{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7fa796068c3f4816bba3444872cd52bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_49f724528d004b7f82f94ee6a144aa07",
              "IPY_MODEL_263232427f014de2adfedef6dee4ed93",
              "IPY_MODEL_4b87c2b44207459697031436c8c3bfe1"
            ],
            "layout": "IPY_MODEL_1d5fbc14b63e49cbaf4f99a8a492958e"
          }
        },
        "49f724528d004b7f82f94ee6a144aa07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e0ef65029fd4956b323d3e0c9a3e5a4",
            "placeholder": "​",
            "style": "IPY_MODEL_93957802759243e9aac83cfb3269f31a",
            "value": "100%"
          }
        },
        "263232427f014de2adfedef6dee4ed93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6f66b1b921f4c8f9ef604fe776a6cdb",
            "max": 102530333,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_76d936e1cd54498faf69cb17130b16b2",
            "value": 102530333
          }
        },
        "4b87c2b44207459697031436c8c3bfe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_700e4690272a4dd18a5f481367a052ea",
            "placeholder": "​",
            "style": "IPY_MODEL_9fc297e78c8d4b1e8c4dcb9aae092278",
            "value": " 97.8M/97.8M [00:01&lt;00:00, 99.4MB/s]"
          }
        },
        "1d5fbc14b63e49cbaf4f99a8a492958e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e0ef65029fd4956b323d3e0c9a3e5a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93957802759243e9aac83cfb3269f31a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6f66b1b921f4c8f9ef604fe776a6cdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76d936e1cd54498faf69cb17130b16b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "700e4690272a4dd18a5f481367a052ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fc297e78c8d4b1e8c4dcb9aae092278": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ba1d072baac498686217f26971cadcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d0b7b52f8aed44788f0d57dc482e354f",
              "IPY_MODEL_46cb9c54cf754084af1dcd06407d2f42",
              "IPY_MODEL_f35d7023ac934d5a8593014cdf57a40e"
            ],
            "layout": "IPY_MODEL_997d910ceecf4676a82030d0c28f84bc"
          }
        },
        "d0b7b52f8aed44788f0d57dc482e354f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c55f0b2940949b3b73000af6716e577",
            "placeholder": "​",
            "style": "IPY_MODEL_98ba963f2328494b9554e3beab134c0d",
            "value": "100%"
          }
        },
        "46cb9c54cf754084af1dcd06407d2f42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f03b8889c1f346fab88aea5732819c3e",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d3835e2f8c24aba95276921b2f6e16a",
            "value": 170498071
          }
        },
        "f35d7023ac934d5a8593014cdf57a40e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e50cf0c445e4f40bfea3c27a97eb8f6",
            "placeholder": "​",
            "style": "IPY_MODEL_104c282e5fb9489eadf79e1a0d1540d5",
            "value": " 170498071/170498071 [00:03&lt;00:00, 49631793.31it/s]"
          }
        },
        "997d910ceecf4676a82030d0c28f84bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c55f0b2940949b3b73000af6716e577": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98ba963f2328494b9554e3beab134c0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f03b8889c1f346fab88aea5732819c3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d3835e2f8c24aba95276921b2f6e16a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0e50cf0c445e4f40bfea3c27a97eb8f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "104c282e5fb9489eadf79e1a0d1540d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## HW3- Problem 2 - Knowledge Distillation \n"
      ],
      "metadata": {
        "id": "2fLBo03uGA7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Full Name: Mohammad Kalbasi\n",
        "2.   Student Number: 401211028"
      ],
      "metadata": {
        "id": "t30vxrIcGIYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor,Lambda\n",
        "from torchvision import datasets, models, transforms\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torchsummary import summary\n",
        "import time\n",
        "import os\n",
        "import copy\n"
      ],
      "metadata": {
        "id": "s1vx0pKPGEYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sn4SsTIV7Afw",
        "outputId": "1bee1b70-4cf5-475c-b205-3b2cdc43afa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading CIFAR 10 data"
      ],
      "metadata": {
        "id": "knVnT84yIIKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform= transforms.Compose([\n",
        "\n",
        "                                  #    transforms.RandomHorizontalFlip(), # FLips the image w.r.t horizontal axis\n",
        "                                   #   transforms.RandomRotation(10),     #Rotates the image to a specified angel\n",
        "                                    #  transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)), #Performs actions like zooms, change shear angles.\n",
        "                                     # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # Set the color params\n",
        "                                      ToTensor(),\n",
        "                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]), # we need this normalization because we use resnet network which are trained on\n",
        "                                     #imagenet, so input data must have same distribution as imagenet, so we set these parameters based on that\n",
        "                                    # we add data augmentation for training \n",
        "    \n",
        ") # we tried data augmentation for first part (linear tuning) but it just made thing worse! so we removed this part\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_data_augment = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform= transforms.Compose([\n",
        "\n",
        "                                      transforms.RandomHorizontalFlip(), # FLips the image w.r.t horizontal axis\n",
        "                                      transforms.RandomRotation(10),     #Rotates the image to a specified angel\n",
        "                                      transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)), #Performs actions like zooms, change shear angles.\n",
        "                                      transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # Set the color params\n",
        "                                      ToTensor(),\n",
        "                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]), # we need this normalization because we use resnet network which are trained on\n",
        "                                     #imagenet, so input data must have same distribution as imagenet, so we set these parameters based on that\n",
        "                                    # we add data augmentation for training \n",
        "    \n",
        ")\n",
        "\n",
        "\n",
        "train_data_224 = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform= transforms.Compose([\n",
        "                                      transforms.Resize((224,224)),\n",
        "                                      ToTensor(),\n",
        "                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]), # we need this normalization because we use resnet network which are trained on\n",
        "                                     #imagenet, so input data must have same distribution as imagenet, so we set these parameters based on that\n",
        "                                    # we add data augmentation for training \n",
        "    \n",
        ") # in this mode we resized image to original imagenet size\n",
        "\n",
        "train_data_show = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform= [ToTensor(),transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])], \n",
        "    \n",
        ") # we use this just for plotting, so it's train data without any augmentation \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "test_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform= transforms.Compose([ToTensor(),transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
        "    \n",
        ") # we don't use any data augmentation for test data!\n",
        "\n",
        "\n",
        "test_data_224 = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform= transforms.Compose([transforms.Resize((224,224)),ToTensor(),transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
        "    \n",
        ") # we don't use any data augmentation for test data!\n",
        "\n",
        "\n",
        "# once again, change mean and variance of data based of Resnet data(imagenet) so we can get best possible outcome"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170,
          "referenced_widgets": [
            "8ba1d072baac498686217f26971cadcc",
            "d0b7b52f8aed44788f0d57dc482e354f",
            "46cb9c54cf754084af1dcd06407d2f42",
            "f35d7023ac934d5a8593014cdf57a40e",
            "997d910ceecf4676a82030d0c28f84bc",
            "3c55f0b2940949b3b73000af6716e577",
            "98ba963f2328494b9554e3beab134c0d",
            "f03b8889c1f346fab88aea5732819c3e",
            "7d3835e2f8c24aba95276921b2f6e16a",
            "0e50cf0c445e4f40bfea3c27a97eb8f6",
            "104c282e5fb9489eadf79e1a0d1540d5"
          ]
        },
        "id": "GwmShH8AIKGE",
        "outputId": "9ed1aacb-fc18-4e00-c69f-ed452eeed084"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ba1d072baac498686217f26971cadcc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "creating data loaders"
      ],
      "metadata": {
        "id": "7fEt_AbOQKq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64 # batch size is hyperparameter, we should test it with different values\n",
        "# for batch size, we also tested 32 and 128, for 32 we got same result as 64 but it was much slower, and for 128, our accuracy dropped compared to other 2 batch sizes, so in the end we\n",
        "# choose 64 for all parts\n",
        "train_data_loader = DataLoader(train_data,batch_size = batch_size,shuffle = True ) # dataloader for train data, shuffle add more variety to our training, resulting in achiving higher accuracy\n",
        "train_data_loader_aug = DataLoader(train_data_augment,batch_size = batch_size,shuffle = True )\n",
        "train_data_loader_224 = DataLoader(train_data_224,batch_size = 64,shuffle = True )\n",
        "test_data_loader = DataLoader(test_data,batch_size = 64,shuffle = False )\n",
        "test_data_loader_224 = DataLoader(test_data_224,batch_size = batch_size,shuffle = False )\n",
        "\n",
        "dataset_sizes = {'train':len(train_data),'val':len(test_data)}\n",
        "#plotting some samples\n",
        "classDict = {0:'plane', 1:'car', 2:'bird', 3:'cat', 4:'deer',\n",
        "             5:'dog', 6:'frog', 7:'horse', 8:'ship', 9:'truck'}\n",
        "fig = plt.figure(figsize = (15,15))\n",
        "for idx in range(9):\n",
        "  random_number = np.random.randint(0,len(train_data))\n",
        "  X_temp = train_data_show.data[random_number]\n",
        "  y_temp = train_data_show.targets[random_number]\n",
        "  ax = fig.add_subplot(3,3,idx + 1,xticks = [],yticks = [])\n",
        "  ax.imshow(np.squeeze(X_temp),cmap = 'gray')\n",
        "  ax.set_title(classDict[int(y_temp)])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 866
        },
        "id": "rpJLbe4LPu_H",
        "outputId": "751f962b-2aea-49a1-8772-7f4ff7ed64b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x1080 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important Note:**\n",
        "\n",
        "in next parts, we train model in two different conditions, in first condition we didn't resize cifar10 images (32 * 32) so when using resnet model with imagenet weights, it doesn't perform that great.\n",
        "but in scond condition, we resize images to 224 * 224 and then train model ( or linear-tune it) so we can see both results."
      ],
      "metadata": {
        "id": "V_X8af5XTTyf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part A) Loading resnet 50 model and linear fine-tuning"
      ],
      "metadata": {
        "id": "VyrPj9z4eQAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_teacher = torchvision.models.resnet50(pretrained=True)\n",
        "for param in model_teacher.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "num_ftrs = model_teacher.fc.in_features\n",
        "model_teacher.fc = nn.Linear(num_ftrs, 10) # we have 10 output class\n",
        "\n",
        "model_teacher = model_teacher.to(device)"
      ],
      "metadata": {
        "id": "3uhB5IDJZOdT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "7fa796068c3f4816bba3444872cd52bf",
            "49f724528d004b7f82f94ee6a144aa07",
            "263232427f014de2adfedef6dee4ed93",
            "4b87c2b44207459697031436c8c3bfe1",
            "1d5fbc14b63e49cbaf4f99a8a492958e",
            "4e0ef65029fd4956b323d3e0c9a3e5a4",
            "93957802759243e9aac83cfb3269f31a",
            "f6f66b1b921f4c8f9ef604fe776a6cdb",
            "76d936e1cd54498faf69cb17130b16b2",
            "700e4690272a4dd18a5f481367a052ea",
            "9fc297e78c8d4b1e8c4dcb9aae092278"
          ]
        },
        "outputId": "15968117-0fc8-4147-9a3d-3f25234c4724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7fa796068c3f4816bba3444872cd52bf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "when loading pretrained model on pytorch, because in their implementation they used Adaptive pooling, we don't need to change input shape! so we give our model our data directly without changing it's dimensions"
      ],
      "metadata": {
        "id": "E633v5NNfxjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# printing model summary\n",
        "summary(model_teacher, (3, 32, 32)) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYZfWXlDYZlg",
        "outputId": "a523ef01-1998-410d-bc14-f0b3e001f209"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
            "              ReLU-3           [-1, 64, 16, 16]               0\n",
            "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
            "            Conv2d-5             [-1, 64, 8, 8]           4,096\n",
            "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
            "              ReLU-7             [-1, 64, 8, 8]               0\n",
            "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
            "             ReLU-10             [-1, 64, 8, 8]               0\n",
            "           Conv2d-11            [-1, 256, 8, 8]          16,384\n",
            "      BatchNorm2d-12            [-1, 256, 8, 8]             512\n",
            "           Conv2d-13            [-1, 256, 8, 8]          16,384\n",
            "      BatchNorm2d-14            [-1, 256, 8, 8]             512\n",
            "             ReLU-15            [-1, 256, 8, 8]               0\n",
            "       Bottleneck-16            [-1, 256, 8, 8]               0\n",
            "           Conv2d-17             [-1, 64, 8, 8]          16,384\n",
            "      BatchNorm2d-18             [-1, 64, 8, 8]             128\n",
            "             ReLU-19             [-1, 64, 8, 8]               0\n",
            "           Conv2d-20             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-21             [-1, 64, 8, 8]             128\n",
            "             ReLU-22             [-1, 64, 8, 8]               0\n",
            "           Conv2d-23            [-1, 256, 8, 8]          16,384\n",
            "      BatchNorm2d-24            [-1, 256, 8, 8]             512\n",
            "             ReLU-25            [-1, 256, 8, 8]               0\n",
            "       Bottleneck-26            [-1, 256, 8, 8]               0\n",
            "           Conv2d-27             [-1, 64, 8, 8]          16,384\n",
            "      BatchNorm2d-28             [-1, 64, 8, 8]             128\n",
            "             ReLU-29             [-1, 64, 8, 8]               0\n",
            "           Conv2d-30             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-31             [-1, 64, 8, 8]             128\n",
            "             ReLU-32             [-1, 64, 8, 8]               0\n",
            "           Conv2d-33            [-1, 256, 8, 8]          16,384\n",
            "      BatchNorm2d-34            [-1, 256, 8, 8]             512\n",
            "             ReLU-35            [-1, 256, 8, 8]               0\n",
            "       Bottleneck-36            [-1, 256, 8, 8]               0\n",
            "           Conv2d-37            [-1, 128, 8, 8]          32,768\n",
            "      BatchNorm2d-38            [-1, 128, 8, 8]             256\n",
            "             ReLU-39            [-1, 128, 8, 8]               0\n",
            "           Conv2d-40            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-41            [-1, 128, 4, 4]             256\n",
            "             ReLU-42            [-1, 128, 4, 4]               0\n",
            "           Conv2d-43            [-1, 512, 4, 4]          65,536\n",
            "      BatchNorm2d-44            [-1, 512, 4, 4]           1,024\n",
            "           Conv2d-45            [-1, 512, 4, 4]         131,072\n",
            "      BatchNorm2d-46            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-47            [-1, 512, 4, 4]               0\n",
            "       Bottleneck-48            [-1, 512, 4, 4]               0\n",
            "           Conv2d-49            [-1, 128, 4, 4]          65,536\n",
            "      BatchNorm2d-50            [-1, 128, 4, 4]             256\n",
            "             ReLU-51            [-1, 128, 4, 4]               0\n",
            "           Conv2d-52            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-53            [-1, 128, 4, 4]             256\n",
            "             ReLU-54            [-1, 128, 4, 4]               0\n",
            "           Conv2d-55            [-1, 512, 4, 4]          65,536\n",
            "      BatchNorm2d-56            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-57            [-1, 512, 4, 4]               0\n",
            "       Bottleneck-58            [-1, 512, 4, 4]               0\n",
            "           Conv2d-59            [-1, 128, 4, 4]          65,536\n",
            "      BatchNorm2d-60            [-1, 128, 4, 4]             256\n",
            "             ReLU-61            [-1, 128, 4, 4]               0\n",
            "           Conv2d-62            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-63            [-1, 128, 4, 4]             256\n",
            "             ReLU-64            [-1, 128, 4, 4]               0\n",
            "           Conv2d-65            [-1, 512, 4, 4]          65,536\n",
            "      BatchNorm2d-66            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-67            [-1, 512, 4, 4]               0\n",
            "       Bottleneck-68            [-1, 512, 4, 4]               0\n",
            "           Conv2d-69            [-1, 128, 4, 4]          65,536\n",
            "      BatchNorm2d-70            [-1, 128, 4, 4]             256\n",
            "             ReLU-71            [-1, 128, 4, 4]               0\n",
            "           Conv2d-72            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-73            [-1, 128, 4, 4]             256\n",
            "             ReLU-74            [-1, 128, 4, 4]               0\n",
            "           Conv2d-75            [-1, 512, 4, 4]          65,536\n",
            "      BatchNorm2d-76            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-77            [-1, 512, 4, 4]               0\n",
            "       Bottleneck-78            [-1, 512, 4, 4]               0\n",
            "           Conv2d-79            [-1, 256, 4, 4]         131,072\n",
            "      BatchNorm2d-80            [-1, 256, 4, 4]             512\n",
            "             ReLU-81            [-1, 256, 4, 4]               0\n",
            "           Conv2d-82            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-83            [-1, 256, 2, 2]             512\n",
            "             ReLU-84            [-1, 256, 2, 2]               0\n",
            "           Conv2d-85           [-1, 1024, 2, 2]         262,144\n",
            "      BatchNorm2d-86           [-1, 1024, 2, 2]           2,048\n",
            "           Conv2d-87           [-1, 1024, 2, 2]         524,288\n",
            "      BatchNorm2d-88           [-1, 1024, 2, 2]           2,048\n",
            "             ReLU-89           [-1, 1024, 2, 2]               0\n",
            "       Bottleneck-90           [-1, 1024, 2, 2]               0\n",
            "           Conv2d-91            [-1, 256, 2, 2]         262,144\n",
            "      BatchNorm2d-92            [-1, 256, 2, 2]             512\n",
            "             ReLU-93            [-1, 256, 2, 2]               0\n",
            "           Conv2d-94            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-95            [-1, 256, 2, 2]             512\n",
            "             ReLU-96            [-1, 256, 2, 2]               0\n",
            "           Conv2d-97           [-1, 1024, 2, 2]         262,144\n",
            "      BatchNorm2d-98           [-1, 1024, 2, 2]           2,048\n",
            "             ReLU-99           [-1, 1024, 2, 2]               0\n",
            "      Bottleneck-100           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-101            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-102            [-1, 256, 2, 2]             512\n",
            "            ReLU-103            [-1, 256, 2, 2]               0\n",
            "          Conv2d-104            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-105            [-1, 256, 2, 2]             512\n",
            "            ReLU-106            [-1, 256, 2, 2]               0\n",
            "          Conv2d-107           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-108           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-109           [-1, 1024, 2, 2]               0\n",
            "      Bottleneck-110           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-111            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-112            [-1, 256, 2, 2]             512\n",
            "            ReLU-113            [-1, 256, 2, 2]               0\n",
            "          Conv2d-114            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-115            [-1, 256, 2, 2]             512\n",
            "            ReLU-116            [-1, 256, 2, 2]               0\n",
            "          Conv2d-117           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-118           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-119           [-1, 1024, 2, 2]               0\n",
            "      Bottleneck-120           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-121            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-122            [-1, 256, 2, 2]             512\n",
            "            ReLU-123            [-1, 256, 2, 2]               0\n",
            "          Conv2d-124            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-125            [-1, 256, 2, 2]             512\n",
            "            ReLU-126            [-1, 256, 2, 2]               0\n",
            "          Conv2d-127           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-128           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-129           [-1, 1024, 2, 2]               0\n",
            "      Bottleneck-130           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-131            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-132            [-1, 256, 2, 2]             512\n",
            "            ReLU-133            [-1, 256, 2, 2]               0\n",
            "          Conv2d-134            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-135            [-1, 256, 2, 2]             512\n",
            "            ReLU-136            [-1, 256, 2, 2]               0\n",
            "          Conv2d-137           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-138           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-139           [-1, 1024, 2, 2]               0\n",
            "      Bottleneck-140           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-141            [-1, 512, 2, 2]         524,288\n",
            "     BatchNorm2d-142            [-1, 512, 2, 2]           1,024\n",
            "            ReLU-143            [-1, 512, 2, 2]               0\n",
            "          Conv2d-144            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-145            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-146            [-1, 512, 1, 1]               0\n",
            "          Conv2d-147           [-1, 2048, 1, 1]       1,048,576\n",
            "     BatchNorm2d-148           [-1, 2048, 1, 1]           4,096\n",
            "          Conv2d-149           [-1, 2048, 1, 1]       2,097,152\n",
            "     BatchNorm2d-150           [-1, 2048, 1, 1]           4,096\n",
            "            ReLU-151           [-1, 2048, 1, 1]               0\n",
            "      Bottleneck-152           [-1, 2048, 1, 1]               0\n",
            "          Conv2d-153            [-1, 512, 1, 1]       1,048,576\n",
            "     BatchNorm2d-154            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-155            [-1, 512, 1, 1]               0\n",
            "          Conv2d-156            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-157            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-158            [-1, 512, 1, 1]               0\n",
            "          Conv2d-159           [-1, 2048, 1, 1]       1,048,576\n",
            "     BatchNorm2d-160           [-1, 2048, 1, 1]           4,096\n",
            "            ReLU-161           [-1, 2048, 1, 1]               0\n",
            "      Bottleneck-162           [-1, 2048, 1, 1]               0\n",
            "          Conv2d-163            [-1, 512, 1, 1]       1,048,576\n",
            "     BatchNorm2d-164            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-165            [-1, 512, 1, 1]               0\n",
            "          Conv2d-166            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-167            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-168            [-1, 512, 1, 1]               0\n",
            "          Conv2d-169           [-1, 2048, 1, 1]       1,048,576\n",
            "     BatchNorm2d-170           [-1, 2048, 1, 1]           4,096\n",
            "            ReLU-171           [-1, 2048, 1, 1]               0\n",
            "      Bottleneck-172           [-1, 2048, 1, 1]               0\n",
            "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
            "          Linear-174                   [-1, 10]          20,490\n",
            "================================================================\n",
            "Total params: 23,528,522\n",
            "Trainable params: 20,490\n",
            "Non-trainable params: 23,508,032\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 5.86\n",
            "Params size (MB): 89.75\n",
            "Estimated Total Size (MB): 95.63\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "as you can see and based on what is wanted from us, we only changed last FC layer so it has 10 neurons in output(number of classes) all other weights don't change and are freezed."
      ],
      "metadata": {
        "id": "fLmYCAP6GJdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_manual(model, criterion, optimizer,scheduler,val_beark, num_epochs=25):\n",
        "  \"\"\"\n",
        "  function for train our model! in this function we use dataloader directly. so we only can use it for our CIFAR10 dataset with our input name.\n",
        "  inputs: \n",
        "      model: input model \n",
        "      criterion: desired loss function\n",
        "      optimizer: our optimizer(!)\n",
        "      scheduler: for changing learning rate after sum epochs\n",
        "      num_epochs: number of epoches\n",
        "      val_beark: threshold for early stopping, if after \"val_beark\" steps our model don't get better, we end procces\n",
        "  output:\n",
        "      model: our trained model!\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  train_acc = []\n",
        "  train_loss = []\n",
        "  test_acc = []\n",
        "  test_loss = []\n",
        "  since = time.time()\n",
        "\n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "  best_acc = 0.0\n",
        "  best_loss = 100000000000\n",
        "  counter_val_beark = 0\n",
        "  for epoch in range(num_epochs):\n",
        "    ### Training\n",
        "    model.train()\n",
        "    loss_train = 0\n",
        "    acc_train = 0\n",
        "    counter = 1\n",
        "    for batch,label in train_data_loader:\n",
        "      # 1. Forward pass\n",
        "      batch = batch.to(device)\n",
        "      label = label.to(device)\n",
        "      outputs = model(batch) # model outputs raw logits \n",
        "      _, preds = torch.max(outputs, 1)\n",
        "\n",
        "      # print(y_logits)\n",
        "      # 2. Calculate loss and accuracy\n",
        "      counter = counter + 1\n",
        "      loss = criterion(outputs, label)\n",
        "      acc_train += torch.sum(preds == label.data)\n",
        "      loss_train = loss_train + loss\n",
        "      # 3. Optimizer zero grad\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # 4. Loss backwards\n",
        "      loss.backward()\n",
        "\n",
        "      # 5. Optimizer step\n",
        "      optimizer.step()\n",
        "    loss_train = loss_train/counter\n",
        "    acc_train = acc_train/dataset_sizes['train']\n",
        "    train_acc.append(acc_train)\n",
        "    train_loss.append(loss_train)\n",
        "    scheduler.step()\n",
        "    ### Testing\n",
        "    loss_test = 0\n",
        "    acc_test = 0\n",
        "    counter = 1\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "      for batch,label in test_data_loader:\n",
        "      # 1. Forward pass\n",
        "        batch = batch.to(device)\n",
        "        label = label.to(device)\n",
        "        output = model(batch) # model outputs raw logits \n",
        "        _, preds = torch.max(output, 1)\n",
        "        loss_test= criterion(output, label.data)\n",
        "        acc_test += torch.sum(preds == label.data)\n",
        "      loss_test = loss_test/counter\n",
        "      acc_test = acc_test/dataset_sizes['val']\n",
        "      test_loss.append(loss_test)\n",
        "      test_acc.append(acc_test)\n",
        "    if acc_test > best_acc:\n",
        "                best_acc = acc_test\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    if loss_test <= best_loss:\n",
        "                best_loss = loss_test\n",
        "                counter_val_beark = 0\n",
        "    if loss_test > best_loss:\n",
        "                counter_val_beark = counter_val_beark + 1\n",
        "                if (counter_val_beark > val_beark):\n",
        "                  print(f\"early stopping happend!\")\n",
        "                  break;\n",
        "\n",
        "    # Print out what's happening\n",
        "    if epoch % 1 == 0:\n",
        "      print(f\"Epoch: {epoch} | Loss: {loss_train:.5f}, Acc: {acc_train:.2f}% | Test Loss: {loss_test:.5f}, Test Acc: {acc_test:.2f}%\") \n",
        "  time_elapsed = time.time() - since\n",
        "  print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "  print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "  # load best model weights\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return model,best_loss,best_acc\n"
      ],
      "metadata": {
        "id": "kQXyB3dsunU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_manual_augment(model, criterion, optimizer,scheduler,val_beark, num_epochs=25):\n",
        "  \"\"\"\n",
        "  function for train our model! in this function we use dataloader directly. so we only can use it for our CIFAR10 dataset with our input name. compared to previous function, \n",
        "  we use augmented dataset here\n",
        "  inputs: \n",
        "      model: input model \n",
        "      criterion: desired loss function\n",
        "      optimizer: our optimizer(!)\n",
        "      scheduler: for changing learning rate after sum epochs\n",
        "      num_epochs: number of epoches\n",
        "      val_beark: threshold for early stopping, if after \"val_beark\" steps our model don't get better, we end procces\n",
        "  output:\n",
        "      model: our trained model!\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  train_acc = []\n",
        "  train_loss = []\n",
        "  test_acc = []\n",
        "  test_loss = []\n",
        "  since = time.time()\n",
        "\n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "  best_acc = 0.0\n",
        "  best_loss = 100000000000\n",
        "  counter_val_beark = 0\n",
        "  for epoch in range(num_epochs):\n",
        "    ### Training\n",
        "    model.train()\n",
        "    loss_train = 0\n",
        "    acc_train = 0\n",
        "    counter = 1\n",
        "    for batch,label in train_data_loader_aug:\n",
        "      # 1. Forward pass\n",
        "      batch = batch.to(device)\n",
        "      label = label.to(device)\n",
        "      outputs = model(batch) # model outputs raw logits \n",
        "      _, preds = torch.max(outputs, 1)\n",
        "\n",
        "      # print(y_logits)\n",
        "      # 2. Calculate loss and accuracy\n",
        "      counter = counter + 1\n",
        "      loss = criterion(outputs, label)\n",
        "      acc_train += torch.sum(preds == label.data)\n",
        "      loss_train = loss_train + loss\n",
        "      # 3. Optimizer zero grad\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # 4. Loss backwards\n",
        "      loss.backward()\n",
        "\n",
        "      # 5. Optimizer step\n",
        "      optimizer.step()\n",
        "    loss_train = loss_train/counter\n",
        "    acc_train = acc_train/dataset_sizes['train']\n",
        "    train_acc.append(acc_train)\n",
        "    train_loss.append(loss_train)\n",
        "    scheduler.step()\n",
        "    ### Testing\n",
        "    loss_test = 0\n",
        "    acc_test = 0\n",
        "    counter = 1\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "      for batch,label in test_data_loader:\n",
        "      # 1. Forward pass\n",
        "        batch = batch.to(device)\n",
        "        label = label.to(device)\n",
        "        output = model(batch) # model outputs raw logits \n",
        "        _, preds = torch.max(output, 1)\n",
        "        loss_test= criterion(output, label.data)\n",
        "        acc_test += torch.sum(preds == label.data)\n",
        "      loss_test = loss_test/counter\n",
        "      acc_test = acc_test/dataset_sizes['val']\n",
        "      test_loss.append(loss_test)\n",
        "      test_acc.append(acc_test)\n",
        "    if acc_test > best_acc:\n",
        "                best_acc = acc_test\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    if loss_test <= best_loss:\n",
        "                best_loss = loss_test\n",
        "                counter_val_beark = 0\n",
        "    if loss_test > best_loss:\n",
        "                counter_val_beark = counter_val_beark + 1\n",
        "                if (counter_val_beark > val_beark):\n",
        "                  print(f\"early stopping happend!\")\n",
        "                  break;\n",
        "\n",
        "    # Print out what's happening\n",
        "    if epoch % 1 == 0:\n",
        "      print(f\"Epoch: {epoch} | Loss: {loss_train:.5f}, Acc: {acc_train:.2f}% | Test Loss: {loss_test:.5f}, Test Acc: {acc_test:.2f}%\") \n",
        "  time_elapsed = time.time() - since\n",
        "  print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "  print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "  # load best model weights\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return model,best_loss,best_acc\n"
      ],
      "metadata": {
        "id": "xMH9Wfw2JJA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that only parameters of final layer are being optimized as\n",
        "# opposed to before.\n",
        "optimizer_conv = optim.SGD(model_teacher.fc.parameters(), lr=0.001, momentum=0.9)\n",
        "# Decay LR by a factor of 0.5 every 20 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=50, gamma=0.5)\n",
        "\n",
        "model_teacher = model_teacher.to(device)\n",
        "model_teacher_trained,best_loss,best_acc = train_model_manual(model_teacher, criterion, optimizer_conv,exp_lr_scheduler,20,\n",
        "                         num_epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kE7pPA4OprmX",
        "outputId": "d1b839e6-890d-46ab-fab0-c04e60668302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 1.70784, Acc: 0.42% | Test Loss: 1.14738, Test Acc: 0.47%\n",
            "Epoch: 1 | Loss: 1.51809, Acc: 0.48% | Test Loss: 0.98939, Test Acc: 0.51%\n",
            "Epoch: 2 | Loss: 1.45934, Acc: 0.50% | Test Loss: 0.90396, Test Acc: 0.52%\n",
            "Epoch: 3 | Loss: 1.43136, Acc: 0.51% | Test Loss: 0.90747, Test Acc: 0.51%\n",
            "Epoch: 4 | Loss: 1.40500, Acc: 0.52% | Test Loss: 0.83687, Test Acc: 0.53%\n",
            "Epoch: 5 | Loss: 1.38605, Acc: 0.52% | Test Loss: 0.83953, Test Acc: 0.52%\n",
            "Epoch: 6 | Loss: 1.36992, Acc: 0.53% | Test Loss: 0.86883, Test Acc: 0.52%\n",
            "Epoch: 7 | Loss: 1.35803, Acc: 0.53% | Test Loss: 0.88759, Test Acc: 0.54%\n",
            "Epoch: 8 | Loss: 1.34831, Acc: 0.54% | Test Loss: 0.90822, Test Acc: 0.54%\n",
            "Epoch: 9 | Loss: 1.34364, Acc: 0.54% | Test Loss: 0.92603, Test Acc: 0.54%\n",
            "Epoch: 10 | Loss: 1.33287, Acc: 0.54% | Test Loss: 0.83325, Test Acc: 0.54%\n",
            "Epoch: 11 | Loss: 1.32929, Acc: 0.54% | Test Loss: 0.91699, Test Acc: 0.53%\n",
            "Epoch: 12 | Loss: 1.32346, Acc: 0.54% | Test Loss: 0.85949, Test Acc: 0.54%\n",
            "Epoch: 13 | Loss: 1.31903, Acc: 0.55% | Test Loss: 0.77094, Test Acc: 0.54%\n",
            "Epoch: 14 | Loss: 1.31298, Acc: 0.55% | Test Loss: 0.81886, Test Acc: 0.54%\n",
            "Epoch: 15 | Loss: 1.30907, Acc: 0.55% | Test Loss: 0.78291, Test Acc: 0.54%\n",
            "Epoch: 16 | Loss: 1.30486, Acc: 0.55% | Test Loss: 0.80880, Test Acc: 0.55%\n",
            "Epoch: 17 | Loss: 1.29999, Acc: 0.55% | Test Loss: 0.85794, Test Acc: 0.54%\n",
            "Epoch: 18 | Loss: 1.30063, Acc: 0.55% | Test Loss: 0.85043, Test Acc: 0.54%\n",
            "Epoch: 19 | Loss: 1.29156, Acc: 0.56% | Test Loss: 0.90981, Test Acc: 0.55%\n",
            "Epoch: 20 | Loss: 1.29439, Acc: 0.55% | Test Loss: 0.73179, Test Acc: 0.54%\n",
            "Epoch: 21 | Loss: 1.29344, Acc: 0.56% | Test Loss: 0.82122, Test Acc: 0.54%\n",
            "Epoch: 22 | Loss: 1.28725, Acc: 0.56% | Test Loss: 0.69558, Test Acc: 0.54%\n",
            "Epoch: 23 | Loss: 1.28794, Acc: 0.56% | Test Loss: 0.76084, Test Acc: 0.55%\n",
            "Epoch: 24 | Loss: 1.28620, Acc: 0.56% | Test Loss: 0.75357, Test Acc: 0.55%\n",
            "Epoch: 25 | Loss: 1.28431, Acc: 0.56% | Test Loss: 0.88182, Test Acc: 0.54%\n",
            "Epoch: 26 | Loss: 1.28203, Acc: 0.56% | Test Loss: 0.86709, Test Acc: 0.56%\n",
            "Epoch: 27 | Loss: 1.27650, Acc: 0.56% | Test Loss: 0.72393, Test Acc: 0.53%\n",
            "Epoch: 28 | Loss: 1.27722, Acc: 0.56% | Test Loss: 0.89849, Test Acc: 0.54%\n",
            "Epoch: 29 | Loss: 1.27513, Acc: 0.56% | Test Loss: 0.84631, Test Acc: 0.55%\n",
            "Epoch: 30 | Loss: 1.27246, Acc: 0.56% | Test Loss: 0.79227, Test Acc: 0.55%\n",
            "Epoch: 31 | Loss: 1.27891, Acc: 0.56% | Test Loss: 0.78141, Test Acc: 0.55%\n",
            "Epoch: 32 | Loss: 1.28054, Acc: 0.56% | Test Loss: 0.72184, Test Acc: 0.55%\n",
            "Epoch: 33 | Loss: 1.27751, Acc: 0.56% | Test Loss: 0.84689, Test Acc: 0.55%\n",
            "Epoch: 34 | Loss: 1.27613, Acc: 0.56% | Test Loss: 0.88346, Test Acc: 0.55%\n",
            "Epoch: 35 | Loss: 1.27564, Acc: 0.56% | Test Loss: 0.80948, Test Acc: 0.54%\n",
            "Epoch: 36 | Loss: 1.27530, Acc: 0.56% | Test Loss: 0.87447, Test Acc: 0.54%\n",
            "Epoch: 37 | Loss: 1.27297, Acc: 0.56% | Test Loss: 0.85672, Test Acc: 0.55%\n",
            "Epoch: 38 | Loss: 1.27000, Acc: 0.56% | Test Loss: 0.88114, Test Acc: 0.55%\n",
            "Epoch: 39 | Loss: 1.27151, Acc: 0.56% | Test Loss: 0.94828, Test Acc: 0.53%\n",
            "Epoch: 40 | Loss: 1.26715, Acc: 0.56% | Test Loss: 0.78602, Test Acc: 0.55%\n",
            "Epoch: 41 | Loss: 1.26876, Acc: 0.56% | Test Loss: 1.01627, Test Acc: 0.55%\n",
            "Epoch: 42 | Loss: 1.26940, Acc: 0.56% | Test Loss: 0.86031, Test Acc: 0.55%\n",
            "early stopping happend!\n",
            "Training complete in 15m 31s\n",
            "Best val Acc: 0.555900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we also try to train teacher model with other optimizer like adam to see if it improve our results or not"
      ],
      "metadata": {
        "id": "ujalXU50z6pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_teacher_adam = torchvision.models.resnet50(pretrained=True)\n",
        "for param in model_teacher_adam.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "num_ftrs = model_teacher.fc.in_features\n",
        "model_teacher_adam.fc = nn.Linear(num_ftrs, 10) # we have 10 output class\n",
        "\n",
        "model_teacher_adam = model_teacher_adam.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=50, gamma=0.5)\n",
        "\n",
        "# Observe that only parameters of final layer are being optimized as\n",
        "# opposed to before.\n",
        "optimizer_conv = optim.Adam(model_teacher_adam.fc.parameters())\n",
        "model_teacher = model_teacher.to(device)\n",
        "model_teacher_trained_adam = train_model_manual(model_teacher_adam, criterion, optimizer_conv,exp_lr_scheduler,10,\n",
        "                         num_epochs=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDIbTRCn0BjD",
        "outputId": "801248c9-bb02-4ada-b491-385a8ae8d80c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 1.63585, Acc: 0.44% | Test Loss: 1.07680, Test Acc: 0.49%\n",
            "Epoch: 1 | Loss: 1.48771, Acc: 0.49% | Test Loss: 0.98233, Test Acc: 0.51%\n",
            "Epoch: 2 | Loss: 1.43607, Acc: 0.51% | Test Loss: 0.78701, Test Acc: 0.51%\n",
            "Epoch: 3 | Loss: 1.41933, Acc: 0.51% | Test Loss: 0.92544, Test Acc: 0.52%\n",
            "Epoch: 4 | Loss: 1.40020, Acc: 0.52% | Test Loss: 0.83567, Test Acc: 0.51%\n",
            "Epoch: 5 | Loss: 1.38825, Acc: 0.53% | Test Loss: 0.81252, Test Acc: 0.52%\n",
            "Epoch: 6 | Loss: 1.37504, Acc: 0.53% | Test Loss: 0.87756, Test Acc: 0.53%\n",
            "Epoch: 7 | Loss: 1.37104, Acc: 0.53% | Test Loss: 0.84385, Test Acc: 0.51%\n",
            "Epoch: 8 | Loss: 1.36669, Acc: 0.53% | Test Loss: 0.83115, Test Acc: 0.53%\n",
            "Epoch: 9 | Loss: 1.37096, Acc: 0.53% | Test Loss: 0.82263, Test Acc: 0.52%\n",
            "Epoch: 10 | Loss: 1.36947, Acc: 0.53% | Test Loss: 1.14659, Test Acc: 0.52%\n",
            "Epoch: 11 | Loss: 1.37745, Acc: 0.53% | Test Loss: 0.76063, Test Acc: 0.53%\n",
            "Epoch: 12 | Loss: 1.35994, Acc: 0.54% | Test Loss: 1.12310, Test Acc: 0.54%\n",
            "Epoch: 13 | Loss: 1.35962, Acc: 0.54% | Test Loss: 0.97495, Test Acc: 0.52%\n",
            "Epoch: 14 | Loss: 1.36087, Acc: 0.54% | Test Loss: 0.94977, Test Acc: 0.53%\n",
            "Epoch: 15 | Loss: 1.35623, Acc: 0.54% | Test Loss: 0.67852, Test Acc: 0.52%\n",
            "Epoch: 16 | Loss: 1.35837, Acc: 0.54% | Test Loss: 1.13796, Test Acc: 0.53%\n",
            "Epoch: 17 | Loss: 1.35013, Acc: 0.54% | Test Loss: 0.84532, Test Acc: 0.53%\n",
            "Epoch: 18 | Loss: 1.34827, Acc: 0.54% | Test Loss: 1.00835, Test Acc: 0.53%\n",
            "Epoch: 19 | Loss: 1.35276, Acc: 0.54% | Test Loss: 1.11383, Test Acc: 0.53%\n",
            "Epoch: 20 | Loss: 1.35605, Acc: 0.54% | Test Loss: 0.92097, Test Acc: 0.52%\n",
            "Epoch: 21 | Loss: 1.35113, Acc: 0.54% | Test Loss: 1.06099, Test Acc: 0.53%\n",
            "Epoch: 22 | Loss: 1.35473, Acc: 0.54% | Test Loss: 1.03357, Test Acc: 0.54%\n",
            "Epoch: 23 | Loss: 1.34731, Acc: 0.55% | Test Loss: 0.83267, Test Acc: 0.52%\n",
            "Epoch: 24 | Loss: 1.35649, Acc: 0.54% | Test Loss: 0.99069, Test Acc: 0.53%\n",
            "Epoch: 25 | Loss: 1.35220, Acc: 0.54% | Test Loss: 0.94506, Test Acc: 0.53%\n",
            "early stopping happend!\n",
            "Training complete in 9m 23s\n",
            "Best val Acc: 0.538500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "so SGD is overall better for training model, so we use model that trained with SGD. end result aren't really good, but they are not bad either, because if our networks was completly random, best accuracy we could achived should be around 0.1 (1/10, 10 is number of classes that we predict) but our accuracy is around  54, which is 5 times more, and it shows that our networks really can learn, remember, we only trained last layer (20,490 parameters) and it's like we use feature extracted for imagenet data ( which has 1000 class, so it should be more general) and image size is smaller in cifar 10 data, so the freezed wheights work suboptimally in this new conditions, but nonehteless, final result are acceptable and network really learns something\n",
        "\n"
      ],
      "metadata": {
        "id": "vprcbpdrZerW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "now lets try fine-tune model when input size is 224*224"
      ],
      "metadata": {
        "id": "62RFV43lUDMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_manual_224(model, criterion, optimizer,scheduler,val_beark, num_epochs=25):\n",
        "  \"\"\"\n",
        "  function for train our model! in this function we use dataloader directly. so we only can use it for our CIFAR10 dataset with our input name. compared to previous function, \n",
        "  we use augmented dataset here and also we resized our image to imagenet data size(224*224)\n",
        "  inputs: \n",
        "      model: input model \n",
        "      criterion: desired loss function\n",
        "      optimizer: our optimizer(!)\n",
        "      scheduler: for changing learning rate after sum epochs\n",
        "      num_epochs: number of epoches\n",
        "      val_beark: threshold for early stopping, if after \"val_beark\" steps our model don't get better, we end procces\n",
        "  output:\n",
        "      model: our trained model!\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  train_acc = []\n",
        "  train_loss = []\n",
        "  test_acc = []\n",
        "  test_loss = []\n",
        "  since = time.time()\n",
        "\n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "  best_acc = 0.0\n",
        "  best_loss = 100000000000\n",
        "  counter_val_beark = 0\n",
        "  for epoch in range(num_epochs):\n",
        "    ### Training\n",
        "    model.train()\n",
        "    loss_train = 0\n",
        "    acc_train = 0\n",
        "    counter = 1\n",
        "    for batch,label in train_data_loader_224:\n",
        "      # 1. Forward pass\n",
        "      batch = batch.to(device)\n",
        "      label = label.to(device)\n",
        "      outputs = model(batch) # model outputs raw logits \n",
        "      _, preds = torch.max(outputs, 1)\n",
        "\n",
        "      # print(y_logits)\n",
        "      # 2. Calculate loss and accuracy\n",
        "      counter = counter + 1\n",
        "      loss = criterion(outputs, label)\n",
        "      acc_train += torch.sum(preds == label.data)\n",
        "      loss_train = loss_train + loss\n",
        "      # 3. Optimizer zero grad\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # 4. Loss backwards\n",
        "      loss.backward()\n",
        "\n",
        "      # 5. Optimizer step\n",
        "      optimizer.step()\n",
        "    loss_train = loss_train/counter\n",
        "    acc_train = acc_train/dataset_sizes['train']\n",
        "    train_acc.append(acc_train)\n",
        "    train_loss.append(loss_train)\n",
        "    scheduler.step()\n",
        "    ### Testing\n",
        "    loss_test = 0\n",
        "    acc_test = 0\n",
        "    counter = 1\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "      for batch,label in test_data_loader_224:\n",
        "      # 1. Forward pass\n",
        "        batch = batch.to(device)\n",
        "        label = label.to(device)\n",
        "        output = model(batch) # model outputs raw logits \n",
        "        _, preds = torch.max(output, 1)\n",
        "        loss_test= criterion(output, label.data)\n",
        "        acc_test += torch.sum(preds == label.data)\n",
        "      loss_test = loss_test/counter\n",
        "      acc_test = acc_test/dataset_sizes['val']\n",
        "      test_loss.append(loss_test)\n",
        "      test_acc.append(acc_test)\n",
        "    if acc_test > best_acc:\n",
        "                best_acc = acc_test\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    if loss_test <= best_loss:\n",
        "                best_loss = loss_test\n",
        "                counter_val_beark = 0\n",
        "    if loss_test > best_loss:\n",
        "                counter_val_beark = counter_val_beark + 1\n",
        "                if (counter_val_beark > val_beark):\n",
        "                  print(f\"early stopping happend!\")\n",
        "                  break;\n",
        "\n",
        "    # Print out what's happening\n",
        "    if epoch % 1 == 0:\n",
        "      print(f\"Epoch: {epoch} | Loss: {loss_train:.5f}, Acc: {acc_train:.2f}% | Test Loss: {loss_test:.5f}, Test Acc: {acc_test:.2f}%\") \n",
        "  time_elapsed = time.time() - since\n",
        "  print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "  print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "  # load best model weights\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return model,best_loss,best_acc\n"
      ],
      "metadata": {
        "id": "nld3-P9UUHUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_teacher_adam_224 = torchvision.models.resnet50(pretrained=True)\n",
        "for param in model_teacher_adam_224.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "num_ftrs = model_teacher_adam_224.fc.in_features\n",
        "model_teacher_adam_224.fc = nn.Linear(num_ftrs, 10) # we have 10 output class\n",
        "\n",
        "model_teacher_adam_224 = model_teacher_adam_224.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_conv = optim.SGD(model_teacher_adam_224.fc.parameters(), lr=0.001, momentum=0.9)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=50, gamma=0.5)\n",
        "\n",
        "# Observe that only parameters of final layer are being optimized as\n",
        "# opposed to before.\n",
        "model_teacher = model_teacher.to(device)\n",
        "model_teacher_trained_224,_,_ = train_model_manual_224(model_teacher_adam_224, criterion, optimizer_conv,exp_lr_scheduler,10,\n",
        "                         num_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqKQ2ArKU5DW",
        "outputId": "de7f2652-fbfb-4086-f36a-281d2c5908e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 0.97713, Acc: 0.72% | Test Loss: 0.59278, Test Acc: 0.78%\n",
            "Epoch: 1 | Loss: 0.67039, Acc: 0.78% | Test Loss: 0.57974, Test Acc: 0.79%\n",
            "Epoch: 2 | Loss: 0.62165, Acc: 0.79% | Test Loss: 0.51825, Test Acc: 0.80%\n",
            "Epoch: 3 | Loss: 0.59862, Acc: 0.80% | Test Loss: 0.50902, Test Acc: 0.81%\n",
            "Epoch: 4 | Loss: 0.57754, Acc: 0.80% | Test Loss: 0.56593, Test Acc: 0.80%\n",
            "Epoch: 5 | Loss: 0.56869, Acc: 0.81% | Test Loss: 0.53700, Test Acc: 0.81%\n",
            "Epoch: 6 | Loss: 0.55939, Acc: 0.81% | Test Loss: 0.56018, Test Acc: 0.81%\n",
            "Epoch: 7 | Loss: 0.55296, Acc: 0.81% | Test Loss: 0.55853, Test Acc: 0.81%\n",
            "Epoch: 8 | Loss: 0.54533, Acc: 0.81% | Test Loss: 0.55834, Test Acc: 0.81%\n",
            "Epoch: 9 | Loss: 0.53740, Acc: 0.81% | Test Loss: 0.53358, Test Acc: 0.82%\n",
            "Training complete in 31m 17s\n",
            "Best val Acc: 0.815500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "as we can see, model performs much better in this case, remember number of trainable parameters doesn't change based of input size, because models have adaptive pooling so their fc layers have same number of elements, the main difference is, when we use original imagenet size, it's more suitable and output of convolutional layer is exactly same as when inputs are from imagenet, so we extract same kind of information and because of that our performance doesn't drop compare to cases where we use different input size.\n",
        "\n",
        "so for other parts of this problem, we first answere questions when input size is 32*32, then we answere them for 224*224 case."
      ],
      "metadata": {
        "id": "OggBulYAppkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part B)**for teacher student training, lets first define softmax with temperature and our cross entropy loss ( and like pytorch, we implement softmax inside our entropy loss, so in model last layer should be linear which it is) and then write a function to train model just based of teacher model, and then see if model really learn anything or not, if we wrote our functions in a correct way it should! so if model learn then we undrestand we defined our functions correct, so then we add other part of loss function and train model with teacher model and real output labels ( input size = 32*32, we test 224*224 input in the end)"
      ],
      "metadata": {
        "id": "6A51iFmVHZx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_manual(X,t):\n",
        "  \"\"\"\n",
        "  manual function for implementing softmax\n",
        "  input:\n",
        "    X:input, each coloumn is seperate data\n",
        "    t:temperature value\n",
        "  output:\n",
        "    y\n",
        "  \"\"\"\n",
        "  \n",
        "  _,m = X.size()\n",
        "  m = int(m)\n",
        "  x_max = (torch.max(X,axis = 1)).values\n",
        "  X = (X.T - x_max).T #adding value doesn't have effect on softmax output but it helpd to stablaize our values!(because it is ways easier to comput exp(-145) compare to exp(145)! one is near zero and one is very big number that we can't store in memory)\n",
        "  exp_X = torch.exp(X/t)\n",
        "  exp_X_sum = torch.sum(exp_X,axis = 1)\n",
        "  exp_X_sum_rep = exp_X_sum.repeat(m,1).T\n",
        "  y = exp_X/exp_X_sum_rep\n",
        "  return y"
      ],
      "metadata": {
        "id": "z1nfdNqt1OAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_manual(yp,y,t):\n",
        "  \"\"\"\n",
        "  manual function for calculating multi class cross entropy loss\n",
        "  input:\n",
        "    y: ground truth label (each coloumn for each sample)\n",
        "    yp = predicted label\n",
        "    t: temperature value when we want to calculate softmax\n",
        "  output:\n",
        "    loss:calculated loss\n",
        "  \n",
        "  \"\"\"\n",
        "  y_s = softmax_manual(y,t)\n",
        "  yp_s = softmax_manual(yp,t)\n",
        "  epsilon =  1e-10 # adding to our test values to solve problem of calculating log of zero!\n",
        "  log_part = torch.log(yp_s+epsilon)\n",
        "  all_error = -1*y_s*log_part\n",
        "  loss = torch.sum(all_error,axis = 1).mean()\n",
        "  return loss\n"
      ],
      "metadata": {
        "id": "57dgb8lSXvPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Hinton_loss(yp,y_model,y,t,a):\n",
        "  \"\"\"\n",
        "  function for calculating loss estated at \"Distilling the Knowledge in a Neural Network\" paper\n",
        "  input:\n",
        "    y: ground truth label (each coloumn for each sample)\n",
        "    yp = predicted label\n",
        "    y_model: predicted label(in linear form, we apply softmax with temperature in this loss function) by teacher model\n",
        "    t: temperature value when we want to calculate softmax\n",
        "    a:ration between two losses\n",
        "  output:\n",
        "    loss:calculated loss\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  cross_ent = nn.CrossEntropyLoss()\n",
        "  loss_first_tem = (1-a) * cross_ent(yp, y)\n",
        "  loss_second_tem = a*(t*t)*cross_entropy_manual(yp,y_model,t)\n",
        "  loss = loss_first_tem + loss_second_tem\n",
        "  return loss\n"
      ],
      "metadata": {
        "id": "EDNdkjSDp0-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to learn just based of teacher model\n",
        "def train_model_manual_just_teacher(model,model_teacher, optimizer,scheduler,val_beark,t, num_epochs=25):\n",
        "  \"\"\"\n",
        "  function for train our model only based of teacher model(and not input labels) in this function we use dataloader directly. so we only can use it for our CIFAR10 dataset with our input name.\n",
        "  inputs: \n",
        "      model: input model \n",
        "      optimizer: our optimizer(!)\n",
        "      scheduler: for changing learning rate after sum epochs\n",
        "      num_epochs: number of epoches\n",
        "      t: temperature parameter\n",
        "      val_beark: threshold for early stopping, if after \"val_beark\" steps our model don't get better, we end procces\n",
        "  output:\n",
        "      model: our trained model!\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  train_acc = []\n",
        "  train_loss = []\n",
        "  test_acc = []\n",
        "  test_loss = []\n",
        "  since = time.time()\n",
        "\n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "  best_acc = 0.0\n",
        "  best_loss = 100000000000\n",
        "  counter_val_beark = 0\n",
        "  for epoch in range(num_epochs):\n",
        "    ### Training\n",
        "    model.train()\n",
        "    loss_train = 0\n",
        "    acc_train = 0\n",
        "    counter = 1\n",
        "    for batch,label in train_data_loader:\n",
        "      # 1. Forward pass\n",
        "      batch = batch.to(device)\n",
        "      label = label.to(device)\n",
        "      outputs = model(batch) # model outputs raw logits \n",
        "      outputs_teacher = model_teacher(batch)\n",
        "      _, preds = torch.max(outputs, 1)\n",
        "\n",
        "      # print(y_logits)\n",
        "      # 2. Calculate loss and accuracy\n",
        "      counter = counter + 1\n",
        "      loss = cross_entropy_manual(outputs, outputs_teacher,t)\n",
        "      acc_train += torch.sum(preds == label.data)\n",
        "      loss_train = loss_train + loss\n",
        "      # 3. Optimizer zero grad\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # 4. Loss backwards\n",
        "      loss.backward()\n",
        "\n",
        "      # 5. Optimizer step\n",
        "      optimizer.step()\n",
        "    loss_train = loss_train/counter\n",
        "    acc_train = acc_train/dataset_sizes['train']\n",
        "    train_acc.append(acc_train)\n",
        "    train_loss.append(loss_train)\n",
        "    scheduler.step()\n",
        "    ### Testing\n",
        "    loss_test = 0\n",
        "    acc_test = 0\n",
        "    counter = 1\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "      for batch,label in test_data_loader:\n",
        "      # 1. Forward pass\n",
        "        batch = batch.to(device)\n",
        "        label = label.to(device)\n",
        "        output = model(batch) # model outputs raw logits \n",
        "        output_teacher = model_teacher(batch)\n",
        "        _, preds = torch.max(output, 1)\n",
        "        loss_test= cross_entropy_manual(output, output_teacher,t)\n",
        "        acc_test += torch.sum(preds == label.data)\n",
        "      loss_test = loss_test/counter\n",
        "      acc_test = acc_test/dataset_sizes['val']\n",
        "      test_loss.append(loss_test)\n",
        "      test_acc.append(acc_test)\n",
        "    if acc_test > best_acc:\n",
        "                best_acc = acc_test\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    if loss_test <= best_loss:\n",
        "                best_loss = loss_test\n",
        "                counter_val_beark = 0\n",
        "    if loss_test > best_loss:\n",
        "                counter_val_beark = counter_val_beark + 1\n",
        "                if (counter_val_beark > val_beark):\n",
        "                  print(f\"early stopping happend!\")\n",
        "                  break;\n",
        "\n",
        "    # Print out what's happening\n",
        "    if epoch % 1 == 0:\n",
        "      print(f\"Epoch: {epoch} | Loss: {loss_train:.5f}, Acc: {acc_train:.2f}% | Test Loss: {loss_test:.5f}, Test Acc: {acc_test:.2f}%\") \n",
        "  time_elapsed = time.time() - since\n",
        "  print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "  print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "  # load best model weights\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return model,best_loss,best_acc\n"
      ],
      "metadata": {
        "id": "ms5bfuPa1OI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make teacher model untrainable\n",
        "for param in model_teacher_trained.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "XfmwU_5gfJHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_res18_just_teacher = models.resnet18(pretrained=False)\n",
        "num_ftrs = model_res18_just_teacher.fc.in_features\n",
        "# Here the size of each output sample is set to 10.\n",
        "model_res18_just_teacher.fc = nn.Linear(num_ftrs, 10)\n",
        "model_res18_just_teacher = model_res18_just_teacher.to(device)\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_res18_just_teacher.parameters(), lr=0.001, momentum=0.9)\n",
        "# Decay LR by a factor of 0.5 every 20 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=50, gamma=0.5)\n",
        "model_res18_just_teacher,best_loss,best_acc = train_model_manual_just_teacher(model_res18_just_teacher,model_teacher_trained, optimizer_ft,exp_lr_scheduler,30,0.5,num_epochs=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqdvsXYzeTHY",
        "outputId": "480a812e-116f-4301-fa41-aa04f4bace9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0 | Loss: 2.12411, Acc: 0.26% | Test Loss: 2.14757, Test Acc: 0.32%\n",
            "Epoch: 1 | Loss: 1.98605, Acc: 0.36% | Test Loss: 2.37797, Test Acc: 0.39%\n",
            "Epoch: 2 | Loss: 1.94286, Acc: 0.40% | Test Loss: 2.23252, Test Acc: 0.40%\n",
            "Epoch: 3 | Loss: 1.90946, Acc: 0.42% | Test Loss: 2.22382, Test Acc: 0.45%\n",
            "Epoch: 4 | Loss: 1.89006, Acc: 0.45% | Test Loss: 2.11122, Test Acc: 0.46%\n",
            "Epoch: 5 | Loss: 1.88009, Acc: 0.46% | Test Loss: 2.10340, Test Acc: 0.48%\n",
            "Epoch: 6 | Loss: 1.86039, Acc: 0.48% | Test Loss: 2.14204, Test Acc: 0.49%\n",
            "Epoch: 7 | Loss: 1.84326, Acc: 0.49% | Test Loss: 2.12546, Test Acc: 0.50%\n",
            "Epoch: 8 | Loss: 1.84442, Acc: 0.49% | Test Loss: 2.17366, Test Acc: 0.50%\n",
            "Epoch: 9 | Loss: 1.82602, Acc: 0.51% | Test Loss: 2.05884, Test Acc: 0.51%\n",
            "Epoch: 10 | Loss: 1.81607, Acc: 0.50% | Test Loss: 2.02579, Test Acc: 0.50%\n",
            "Epoch: 11 | Loss: 1.81008, Acc: 0.51% | Test Loss: 2.09509, Test Acc: 0.51%\n",
            "Epoch: 12 | Loss: 1.80250, Acc: 0.52% | Test Loss: 2.17282, Test Acc: 0.52%\n",
            "Epoch: 13 | Loss: 1.79836, Acc: 0.53% | Test Loss: 2.20389, Test Acc: 0.53%\n",
            "Epoch: 14 | Loss: 1.79102, Acc: 0.54% | Test Loss: 2.15147, Test Acc: 0.53%\n",
            "Epoch: 15 | Loss: 1.78465, Acc: 0.53% | Test Loss: 2.14045, Test Acc: 0.51%\n",
            "Epoch: 16 | Loss: 1.77255, Acc: 0.54% | Test Loss: 2.20232, Test Acc: 0.53%\n",
            "Epoch: 17 | Loss: 1.77212, Acc: 0.54% | Test Loss: 2.14715, Test Acc: 0.53%\n",
            "Epoch: 18 | Loss: 1.76095, Acc: 0.55% | Test Loss: 2.18694, Test Acc: 0.53%\n",
            "Epoch: 19 | Loss: 1.75292, Acc: 0.55% | Test Loss: 2.19077, Test Acc: 0.51%\n",
            "Epoch: 20 | Loss: 1.75223, Acc: 0.55% | Test Loss: 2.12534, Test Acc: 0.54%\n",
            "Epoch: 21 | Loss: 1.74123, Acc: 0.56% | Test Loss: 2.04212, Test Acc: 0.55%\n",
            "Epoch: 22 | Loss: 1.73662, Acc: 0.55% | Test Loss: 2.07069, Test Acc: 0.53%\n",
            "Epoch: 23 | Loss: 1.73081, Acc: 0.56% | Test Loss: 2.06322, Test Acc: 0.54%\n",
            "Epoch: 24 | Loss: 1.73247, Acc: 0.55% | Test Loss: 2.13126, Test Acc: 0.54%\n",
            "Epoch: 25 | Loss: 1.72215, Acc: 0.54% | Test Loss: 2.13135, Test Acc: 0.51%\n",
            "Epoch: 26 | Loss: 1.72305, Acc: 0.55% | Test Loss: 2.07846, Test Acc: 0.54%\n",
            "Epoch: 27 | Loss: 1.71330, Acc: 0.55% | Test Loss: 2.10847, Test Acc: 0.54%\n",
            "Epoch: 28 | Loss: 1.71038, Acc: 0.56% | Test Loss: 2.04660, Test Acc: 0.56%\n",
            "Epoch: 29 | Loss: 1.70614, Acc: 0.57% | Test Loss: 2.18798, Test Acc: 0.57%\n",
            "Training complete in 14m 33s\n",
            "Best val Acc: 0.572600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "so we can see that only with using teacher model, we can achive same (and even better!) results as our teacher model so it should work! now we define our complete loss function and find best parameters, and then train whole network with that"
      ],
      "metadata": {
        "id": "taL2GX6jpVls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Hinton_loss(yp,y_model,y,t,a):\n",
        "  \"\"\"\n",
        "  function for calculating loss estated at \"Distilling the Knowledge in a Neural Network\" paper\n",
        "  input:\n",
        "    y: ground truth label (each coloumn for each sample)\n",
        "    yp = predicted label\n",
        "    y_model: predicted label(in linear form, we apply softmax with temperature in this loss function) by teacher model\n",
        "    t: temperature value when we want to calculate softmax\n",
        "    a:ration between two losses\n",
        "  output:\n",
        "    loss:calculated loss\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  cross_ent = nn.CrossEntropyLoss()\n",
        "  loss_first_tem = (1-a) * cross_ent(yp, y)\n",
        "  loss_second_tem = a*(t*t)*cross_entropy_manual(yp,y_model,t)\n",
        "  loss = loss_first_tem + loss_second_tem\n",
        "  return loss\n"
      ],
      "metadata": {
        "id": "0vN8x6KLpp3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to learn based of teacher model and real labels\n",
        "def train_model_manual_Hinton(model,model_teacher, optimizer,scheduler,val_beark,t,a, num_epochs=25):\n",
        "  \"\"\"\n",
        "  function for train our model only based of teacher model(and not input labels) in this function we use dataloader directly. so we only can use it for our CIFAR10 dataset with our input name.\n",
        "  inputs: \n",
        "      model: input model \n",
        "      optimizer: our optimizer(!)\n",
        "      scheduler: for changing learning rate after sum epochs\n",
        "      num_epochs: number of epoches\n",
        "      t: temperature parameter\n",
        "      a: ration between two losses (it value should be between 0 and 1)\n",
        "      val_beark: threshold for early stopping, if after \"val_beark\" steps our model don't get better, we end procces\n",
        "  output:\n",
        "      model: our trained model!\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  train_acc = []\n",
        "  train_loss = []\n",
        "  test_acc = []\n",
        "  test_loss = []\n",
        "  since = time.time()\n",
        "\n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "  best_acc = 0.0\n",
        "  best_loss = 100000000000\n",
        "  counter_val_beark = 0\n",
        "  for epoch in range(num_epochs):\n",
        "    ### Training\n",
        "    model.train()\n",
        "    loss_train = 0\n",
        "    acc_train = 0\n",
        "    counter = 1\n",
        "    for batch,label in train_data_loader:\n",
        "      # 1. Forward pass\n",
        "      batch = batch.to(device)\n",
        "      label = label.to(device)\n",
        "      outputs = model(batch) # model outputs raw logits \n",
        "      outputs_teacher = model_teacher(batch)\n",
        "      _, preds = torch.max(outputs, 1)\n",
        "\n",
        "      # print(y_logits)\n",
        "      # 2. Calculate loss and accuracy\n",
        "      counter = counter + 1\n",
        "      loss = Hinton_loss(outputs, outputs_teacher,label.data,t,a)\n",
        "      acc_train += torch.sum(preds == label.data)\n",
        "      loss_train = loss_train + loss\n",
        "      # 3. Optimizer zero grad\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # 4. Loss backwards\n",
        "      loss.backward()\n",
        "\n",
        "      # 5. Optimizer step\n",
        "      optimizer.step()\n",
        "    loss_train = loss_train/counter\n",
        "    acc_train = acc_train/dataset_sizes['train']\n",
        "    train_acc.append(acc_train)\n",
        "    train_loss.append(loss_train)\n",
        "    scheduler.step()\n",
        "    ### Testing\n",
        "    loss_test = 0\n",
        "    acc_test = 0\n",
        "    counter = 1\n",
        "\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "      for batch,label in test_data_loader:\n",
        "      # 1. Forward pass\n",
        "        batch = batch.to(device)\n",
        "        label = label.to(device)\n",
        "        output = model(batch) # model outputs raw logits \n",
        "        output_teacher = model_teacher(batch)\n",
        "        _, preds = torch.max(output, 1)\n",
        "        loss_test= Hinton_loss(output, output_teacher,label.data,t,a)\n",
        "        acc_test += torch.sum(preds == label.data)\n",
        "      loss_test = loss_test/counter\n",
        "      acc_test = acc_test/dataset_sizes['val']\n",
        "      test_loss.append(loss_test)\n",
        "      test_acc.append(acc_test)\n",
        "    if acc_test > best_acc:\n",
        "                best_acc = acc_test\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    if loss_test <= best_loss:\n",
        "                best_loss = loss_test\n",
        "                counter_val_beark = 0\n",
        "    if loss_test > best_loss:\n",
        "                counter_val_beark = counter_val_beark + 1\n",
        "                if (counter_val_beark > val_beark):\n",
        "                  print(f\"early stopping happend!\")\n",
        "                  break;\n",
        "\n",
        "    # Print out what's happening\n",
        "    if epoch % 1 == 0:\n",
        "      print(f\"Epoch: {epoch} | Loss: {loss_train:.5f}, Acc: {acc_train:.2f}% | Test Loss: {loss_test:.5f}, Test Acc: {acc_test:.2f}%\") \n",
        "  time_elapsed = time.time() - since\n",
        "  print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "  print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "  # load best model weights\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return model,best_loss,best_acc\n"
      ],
      "metadata": {
        "id": "nFa9Axn6raEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now we should find betst values for our two hyperparameters alpha and t. for this purpose we only run training process for just 3 epoches and see results"
      ],
      "metadata": {
        "id": "IS8rUmg3sDAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([0,0.25,0.5,0.75,1], dtype=torch.float64, device=device)\n",
        "t = torch.tensor([0.01,0.1,0.5,1,10,100], dtype=torch.float64, device=device)\n",
        "score_mat = np.zeros((len(a),len(t)))\n",
        "for i in range(len(a)):\n",
        "  for j in range(len(t)):\n",
        "    model_res18_Hinton = models.resnet18(pretrained=False)\n",
        "    num_ftrs = model_res18_Hinton.fc.in_features\n",
        "    # Here the size of each output sample is set to 10.\n",
        "    model_res18_Hinton.fc = nn.Linear(num_ftrs, 10)\n",
        "    model_res18_Hinton = model_res18_Hinton.to(device)\n",
        "\n",
        "    print(f\"training model for a = {a[i]}, t = {t[j]}\")\n",
        "    print(100*'*')\n",
        "    optimizer_ft = optim.SGD(model_res18_Hinton.parameters(), lr=0.001, momentum=0.9)\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=50, gamma=0.5)\n",
        "    _,_,score_mat[i,j] = train_model_manual_Hinton(model_res18_Hinton,model_teacher_trained, optimizer_ft,exp_lr_scheduler,30,t[j],a[i],num_epochs=3)\n",
        "    print(100*'*')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwPC-fRDsXF8",
        "outputId": "f2f1dea5-f0af-44c2-c21d-d30e900aeb17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training model for a = 0.0, t = 0.01\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 1.57947, Acc: 0.43% | Test Loss: 0.73061, Test Acc: 0.51%\n",
            "Epoch: 1 | Loss: 1.22835, Acc: 0.56% | Test Loss: 0.86278, Test Acc: 0.58%\n",
            "Epoch: 2 | Loss: 1.03689, Acc: 0.63% | Test Loss: 0.75883, Test Acc: 0.61%\n",
            "Training complete in 1m 30s\n",
            "Best val Acc: 0.611500\n",
            "****************************************************************************************************\n",
            "training model for a = 0.0, t = 0.1\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 1.59119, Acc: 0.42% | Test Loss: 0.97321, Test Acc: 0.51%\n",
            "Epoch: 1 | Loss: 1.22993, Acc: 0.56% | Test Loss: 0.79413, Test Acc: 0.58%\n",
            "Epoch: 2 | Loss: 1.03009, Acc: 0.63% | Test Loss: 0.67753, Test Acc: 0.60%\n",
            "Training complete in 1m 29s\n",
            "Best val Acc: 0.595200\n",
            "****************************************************************************************************\n",
            "training model for a = 0.0, t = 0.5\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 1.59578, Acc: 0.42% | Test Loss: 1.10734, Test Acc: 0.51%\n",
            "Epoch: 1 | Loss: 1.24082, Acc: 0.56% | Test Loss: 0.68956, Test Acc: 0.56%\n",
            "Epoch: 2 | Loss: 1.05334, Acc: 0.62% | Test Loss: 0.70128, Test Acc: 0.61%\n",
            "Training complete in 1m 29s\n",
            "Best val Acc: 0.607000\n",
            "****************************************************************************************************\n",
            "training model for a = 0.0, t = 1.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 1.59953, Acc: 0.42% | Test Loss: 1.11885, Test Acc: 0.51%\n",
            "Epoch: 1 | Loss: 1.24954, Acc: 0.55% | Test Loss: 1.09131, Test Acc: 0.57%\n",
            "Epoch: 2 | Loss: 1.05574, Acc: 0.62% | Test Loss: 1.04400, Test Acc: 0.61%\n",
            "Training complete in 1m 30s\n",
            "Best val Acc: 0.610400\n",
            "****************************************************************************************************\n",
            "training model for a = 0.0, t = 10.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 1.58453, Acc: 0.42% | Test Loss: 0.98957, Test Acc: 0.52%\n",
            "Epoch: 1 | Loss: 1.21632, Acc: 0.56% | Test Loss: 0.97913, Test Acc: 0.58%\n",
            "Epoch: 2 | Loss: 1.01719, Acc: 0.64% | Test Loss: 0.83703, Test Acc: 0.62%\n",
            "Training complete in 1m 28s\n",
            "Best val Acc: 0.618100\n",
            "****************************************************************************************************\n",
            "training model for a = 0.0, t = 100.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 1.59621, Acc: 0.42% | Test Loss: 1.00145, Test Acc: 0.52%\n",
            "Epoch: 1 | Loss: 1.23714, Acc: 0.55% | Test Loss: 1.22920, Test Acc: 0.57%\n",
            "Epoch: 2 | Loss: 1.04081, Acc: 0.63% | Test Loss: 0.89618, Test Acc: 0.61%\n",
            "Training complete in 1m 28s\n",
            "Best val Acc: 0.611800\n",
            "****************************************************************************************************\n",
            "training model for a = 0.25, t = 0.01\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 1.21591, Acc: 0.41% | Test Loss: 0.97217, Test Acc: 0.50%\n",
            "Epoch: 1 | Loss: 0.95147, Acc: 0.54% | Test Loss: 0.80117, Test Acc: 0.56%\n",
            "Epoch: 2 | Loss: 0.80529, Acc: 0.62% | Test Loss: 0.78270, Test Acc: 0.60%\n",
            "Training complete in 1m 29s\n",
            "Best val Acc: 0.599400\n",
            "****************************************************************************************************\n",
            "training model for a = 0.25, t = 0.1\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 1.22012, Acc: 0.41% | Test Loss: 0.91077, Test Acc: 0.50%\n",
            "Epoch: 1 | Loss: 0.95543, Acc: 0.55% | Test Loss: 0.84199, Test Acc: 0.56%\n",
            "Epoch: 2 | Loss: 0.81833, Acc: 0.61% | Test Loss: 0.62002, Test Acc: 0.59%\n",
            "Training complete in 1m 30s\n",
            "Best val Acc: 0.589400\n",
            "****************************************************************************************************\n",
            "training model for a = 0.25, t = 0.5\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 1.35154, Acc: 0.41% | Test Loss: 1.25515, Test Acc: 0.50%\n",
            "Epoch: 1 | Loss: 1.09677, Acc: 0.54% | Test Loss: 0.97149, Test Acc: 0.55%\n",
            "Epoch: 2 | Loss: 0.96230, Acc: 0.61% | Test Loss: 1.22152, Test Acc: 0.58%\n",
            "Training complete in 1m 28s\n",
            "Best val Acc: 0.581500\n",
            "****************************************************************************************************\n",
            "training model for a = 0.25, t = 1.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 1.74609, Acc: 0.41% | Test Loss: 1.62202, Test Acc: 0.50%\n",
            "Epoch: 1 | Loss: 1.48760, Acc: 0.54% | Test Loss: 1.69007, Test Acc: 0.56%\n",
            "Epoch: 2 | Loss: 1.36378, Acc: 0.60% | Test Loss: 1.35792, Test Acc: 0.59%\n",
            "Training complete in 1m 29s\n",
            "Best val Acc: 0.592700\n",
            "****************************************************************************************************\n",
            "training model for a = 0.25, t = 10.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 58.63582, Acc: 0.28% | Test Loss: 53.19729, Test Acc: 0.37%\n",
            "Epoch: 1 | Loss: 52.90804, Acc: 0.35% | Test Loss: 55.57318, Test Acc: 0.37%\n",
            "Epoch: 2 | Loss: 50.35089, Acc: 0.41% | Test Loss: 52.52651, Test Acc: 0.41%\n",
            "Training complete in 1m 28s\n",
            "Best val Acc: 0.413500\n",
            "****************************************************************************************************\n",
            "training model for a = 0.25, t = 100.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 1 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 2 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Training complete in 1m 28s\n",
            "Best val Acc: 0.100000\n",
            "****************************************************************************************************\n",
            "training model for a = 0.5, t = 0.01\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 0.83808, Acc: 0.39% | Test Loss: 0.70098, Test Acc: 0.48%\n",
            "Epoch: 1 | Loss: 0.65534, Acc: 0.53% | Test Loss: 0.60347, Test Acc: 0.53%\n",
            "Epoch: 2 | Loss: 0.56802, Acc: 0.60% | Test Loss: 0.44181, Test Acc: 0.57%\n",
            "Training complete in 1m 31s\n",
            "Best val Acc: 0.572100\n",
            "****************************************************************************************************\n",
            "training model for a = 0.5, t = 0.1\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 0.84968, Acc: 0.39% | Test Loss: 0.57023, Test Acc: 0.48%\n",
            "Epoch: 1 | Loss: 0.67324, Acc: 0.52% | Test Loss: 0.48928, Test Acc: 0.54%\n",
            "Epoch: 2 | Loss: 0.58589, Acc: 0.59% | Test Loss: 0.45952, Test Acc: 0.57%\n",
            "Training complete in 1m 28s\n",
            "Best val Acc: 0.571500\n",
            "****************************************************************************************************\n",
            "training model for a = 0.5, t = 0.5\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 1.09563, Acc: 0.39% | Test Loss: 0.93161, Test Acc: 0.49%\n",
            "Epoch: 1 | Loss: 0.92703, Acc: 0.52% | Test Loss: 0.88044, Test Acc: 0.54%\n",
            "Epoch: 2 | Loss: 0.84992, Acc: 0.58% | Test Loss: 0.78111, Test Acc: 0.56%\n",
            "Training complete in 1m 28s\n",
            "Best val Acc: 0.558600\n",
            "****************************************************************************************************\n",
            "training model for a = 0.5, t = 1.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 1.90371, Acc: 0.37% | Test Loss: 1.84591, Test Acc: 0.45%\n",
            "Epoch: 1 | Loss: 1.71435, Acc: 0.48% | Test Loss: 1.79612, Test Acc: 0.50%\n",
            "Epoch: 2 | Loss: 1.63609, Acc: 0.53% | Test Loss: 1.79897, Test Acc: 0.53%\n",
            "Training complete in 1m 30s\n",
            "Best val Acc: 0.534600\n",
            "****************************************************************************************************\n",
            "training model for a = 0.5, t = 10.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: nan, Acc: 0.12% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 1 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 2 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Training complete in 1m 28s\n",
            "Best val Acc: 0.100000\n",
            "****************************************************************************************************\n",
            "training model for a = 0.5, t = 100.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 1 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 2 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Training complete in 1m 31s\n",
            "Best val Acc: 0.100000\n",
            "****************************************************************************************************\n",
            "training model for a = 0.75, t = 0.01\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 0.44843, Acc: 0.35% | Test Loss: 0.33276, Test Acc: 0.44%\n",
            "Epoch: 1 | Loss: 0.36034, Acc: 0.48% | Test Loss: 0.29390, Test Acc: 0.50%\n",
            "Epoch: 2 | Loss: 0.32185, Acc: 0.54% | Test Loss: 0.28395, Test Acc: 0.53%\n",
            "Training complete in 1m 28s\n",
            "Best val Acc: 0.532600\n",
            "****************************************************************************************************\n",
            "training model for a = 0.75, t = 0.1\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 0.46358, Acc: 0.35% | Test Loss: 0.32448, Test Acc: 0.45%\n",
            "Epoch: 1 | Loss: 0.37447, Acc: 0.48% | Test Loss: 0.32487, Test Acc: 0.50%\n",
            "Epoch: 2 | Loss: 0.33476, Acc: 0.54% | Test Loss: 0.27216, Test Acc: 0.54%\n",
            "Training complete in 1m 28s\n",
            "Best val Acc: 0.537300\n",
            "****************************************************************************************************\n",
            "training model for a = 0.75, t = 0.5\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 0.84375, Acc: 0.34% | Test Loss: 0.81934, Test Acc: 0.44%\n",
            "Epoch: 1 | Loss: 0.74831, Acc: 0.46% | Test Loss: 0.79563, Test Acc: 0.49%\n",
            "Epoch: 2 | Loss: 0.71214, Acc: 0.51% | Test Loss: 0.77063, Test Acc: 0.51%\n",
            "Training complete in 1m 29s\n",
            "Best val Acc: 0.513900\n",
            "****************************************************************************************************\n",
            "training model for a = 0.75, t = 1.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 2.01083, Acc: 0.34% | Test Loss: 1.97564, Test Acc: 0.41%\n",
            "Epoch: 1 | Loss: 1.86849, Acc: 0.43% | Test Loss: 1.90431, Test Acc: 0.46%\n",
            "Epoch: 2 | Loss: 1.81687, Acc: 0.48% | Test Loss: 1.89662, Test Acc: 0.47%\n",
            "Training complete in 1m 28s\n",
            "Best val Acc: 0.471300\n",
            "****************************************************************************************************\n",
            "training model for a = 0.75, t = 10.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 1 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 2 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Training complete in 1m 28s\n",
            "Best val Acc: 0.100000\n",
            "****************************************************************************************************\n",
            "training model for a = 0.75, t = 100.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 1 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 2 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Training complete in 1m 31s\n",
            "Best val Acc: 0.100000\n",
            "****************************************************************************************************\n",
            "training model for a = 1.0, t = 0.01\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 0.00024, Acc: 0.10% | Test Loss: 0.00024, Test Acc: 0.10%\n",
            "Epoch: 1 | Loss: 0.00024, Acc: 0.10% | Test Loss: 0.00024, Test Acc: 0.10%\n",
            "Epoch: 2 | Loss: 0.00024, Acc: 0.10% | Test Loss: 0.00024, Test Acc: 0.10%\n",
            "Training complete in 1m 29s\n",
            "Best val Acc: 0.102100\n",
            "****************************************************************************************************\n",
            "training model for a = 1.0, t = 0.1\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 0.02379, Acc: 0.13% | Test Loss: 0.02596, Test Acc: 0.15%\n",
            "Epoch: 1 | Loss: 0.02286, Acc: 0.17% | Test Loss: 0.02261, Test Acc: 0.19%\n",
            "Epoch: 2 | Loss: 0.02228, Acc: 0.20% | Test Loss: 0.02162, Test Acc: 0.21%\n",
            "Training complete in 1m 29s\n",
            "Best val Acc: 0.208500\n",
            "****************************************************************************************************\n",
            "training model for a = 1.0, t = 0.5\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 0.54010, Acc: 0.23% | Test Loss: 0.51870, Test Acc: 0.31%\n",
            "Epoch: 1 | Loss: 0.50551, Acc: 0.33% | Test Loss: 0.49741, Test Acc: 0.37%\n",
            "Epoch: 2 | Loss: 0.49429, Acc: 0.37% | Test Loss: 0.51047, Test Acc: 0.39%\n",
            "Training complete in 1m 31s\n",
            "Best val Acc: 0.386600\n",
            "****************************************************************************************************\n",
            "training model for a = 1.0, t = 1.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 2.11554, Acc: 0.29% | Test Loss: 2.19795, Test Acc: 0.37%\n",
            "Epoch: 1 | Loss: 1.98048, Acc: 0.37% | Test Loss: 2.18597, Test Acc: 0.39%\n",
            "Epoch: 2 | Loss: 1.93876, Acc: 0.40% | Test Loss: 2.31811, Test Acc: 0.42%\n",
            "Training complete in 1m 28s\n",
            "Best val Acc: 0.417000\n",
            "****************************************************************************************************\n",
            "training model for a = 1.0, t = 10.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 1 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 2 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Training complete in 1m 29s\n",
            "Best val Acc: 0.100000\n",
            "****************************************************************************************************\n",
            "training model for a = 1.0, t = 100.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 1 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 2 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Training complete in 1m 52s\n",
            "Best val Acc: 0.100000\n",
            "****************************************************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based of result, it seem our model works best when teacher model doesn't have significant impact. in next part we show that if we train resnet18 model from scratch, we get better result than linear tuning just resnet50 (pretrained on imagenet). in previous part we tried different methods to imrove teacher model accuracy (testing different batch sizes and also adding data augmentation) but non of them really helped.\n",
        "so for this part, we choose a = 0.25 , t = 0.5 so our teacher model can have impact on final results, but after part D (fine tuning whole resnet50 model) we try this method with a much better teacher model and show this method really works if our teacher have a better accuracy than our normal model"
      ],
      "metadata": {
        "id": "gE11UbSp9_pO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_res18_Hinton = models.resnet18(pretrained=False)\n",
        "num_ftrs = model_res18_Hinton.fc.in_features\n",
        "# Here the size of each output sample is set to 10.\n",
        "model_res18_Hinton.fc = nn.Linear(num_ftrs, 10)\n",
        "model_res18_Hinton = model_res18_Hinton.to(device)\n",
        "\n",
        "print(f\"training model for a = {a[1]}, t = {t[2]}\")\n",
        "print(100*'*')\n",
        "optimizer_ft = optim.SGD(model_res18_Hinton.parameters(), lr=0.001, momentum=0.9)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=50, gamma=0.5)\n",
        "model_res18_Hinton_trained,_,_ = train_model_manual_Hinton(model_res18_Hinton,model_teacher_trained, optimizer_ft,exp_lr_scheduler,30,t[2],a[1],num_epochs=200) #a[1] = 0.25 and t[2] = 0.5 our selected parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFU1lqdP_ZDq",
        "outputId": "96ce6e20-ed8f-421c-b96c-9797ebc377dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training model for a = 0.25, t = 0.5\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 1.35573, Acc: 0.41% | Test Loss: 1.00607, Test Acc: 0.50%\n",
            "Epoch: 1 | Loss: 1.08962, Acc: 0.54% | Test Loss: 0.92286, Test Acc: 0.57%\n",
            "Epoch: 2 | Loss: 0.95218, Acc: 0.62% | Test Loss: 1.10497, Test Acc: 0.60%\n",
            "Epoch: 3 | Loss: 0.84297, Acc: 0.67% | Test Loss: 0.89607, Test Acc: 0.62%\n",
            "Epoch: 4 | Loss: 0.75240, Acc: 0.72% | Test Loss: 0.78113, Test Acc: 0.63%\n",
            "Epoch: 5 | Loss: 0.67083, Acc: 0.77% | Test Loss: 0.85513, Test Acc: 0.64%\n",
            "Epoch: 6 | Loss: 0.58968, Acc: 0.81% | Test Loss: 0.91541, Test Acc: 0.65%\n",
            "Epoch: 7 | Loss: 0.51877, Acc: 0.85% | Test Loss: 0.69933, Test Acc: 0.65%\n",
            "Epoch: 8 | Loss: 0.45390, Acc: 0.89% | Test Loss: 0.81183, Test Acc: 0.65%\n",
            "Epoch: 9 | Loss: 0.40264, Acc: 0.92% | Test Loss: 0.73212, Test Acc: 0.66%\n",
            "Epoch: 10 | Loss: 0.36359, Acc: 0.94% | Test Loss: 0.96961, Test Acc: 0.65%\n",
            "Epoch: 11 | Loss: 0.33453, Acc: 0.96% | Test Loss: 0.84674, Test Acc: 0.65%\n",
            "Epoch: 12 | Loss: 0.30999, Acc: 0.97% | Test Loss: 0.90807, Test Acc: 0.66%\n",
            "Epoch: 13 | Loss: 0.29168, Acc: 0.98% | Test Loss: 0.74626, Test Acc: 0.66%\n",
            "Epoch: 14 | Loss: 0.28024, Acc: 0.99% | Test Loss: 0.77318, Test Acc: 0.67%\n",
            "Epoch: 15 | Loss: 0.27086, Acc: 0.99% | Test Loss: 0.67454, Test Acc: 0.66%\n",
            "Epoch: 16 | Loss: 0.26504, Acc: 0.99% | Test Loss: 0.79720, Test Acc: 0.66%\n",
            "Epoch: 17 | Loss: 0.26107, Acc: 0.99% | Test Loss: 0.84326, Test Acc: 0.67%\n",
            "Epoch: 18 | Loss: 0.25662, Acc: 0.99% | Test Loss: 0.77573, Test Acc: 0.67%\n",
            "Epoch: 19 | Loss: 0.25118, Acc: 1.00% | Test Loss: 0.76016, Test Acc: 0.67%\n",
            "Epoch: 20 | Loss: 0.24749, Acc: 1.00% | Test Loss: 0.56316, Test Acc: 0.67%\n",
            "Epoch: 21 | Loss: 0.24532, Acc: 1.00% | Test Loss: 0.67995, Test Acc: 0.67%\n",
            "Epoch: 22 | Loss: 0.24182, Acc: 1.00% | Test Loss: 0.74522, Test Acc: 0.67%\n",
            "Epoch: 23 | Loss: 0.24043, Acc: 1.00% | Test Loss: 0.71069, Test Acc: 0.67%\n",
            "Epoch: 24 | Loss: 0.23815, Acc: 1.00% | Test Loss: 0.74714, Test Acc: 0.67%\n",
            "Epoch: 25 | Loss: 0.23553, Acc: 1.00% | Test Loss: 0.70778, Test Acc: 0.67%\n",
            "Epoch: 26 | Loss: 0.23323, Acc: 1.00% | Test Loss: 0.76518, Test Acc: 0.67%\n",
            "Epoch: 27 | Loss: 0.23174, Acc: 1.00% | Test Loss: 0.63901, Test Acc: 0.67%\n",
            "Epoch: 28 | Loss: 0.23220, Acc: 1.00% | Test Loss: 0.65821, Test Acc: 0.67%\n",
            "Epoch: 29 | Loss: 0.22982, Acc: 1.00% | Test Loss: 0.71726, Test Acc: 0.67%\n",
            "Epoch: 30 | Loss: 0.22802, Acc: 1.00% | Test Loss: 0.73281, Test Acc: 0.67%\n",
            "Epoch: 31 | Loss: 0.22684, Acc: 1.00% | Test Loss: 0.78221, Test Acc: 0.67%\n",
            "Epoch: 32 | Loss: 0.22544, Acc: 1.00% | Test Loss: 0.69016, Test Acc: 0.67%\n",
            "Epoch: 33 | Loss: 0.22487, Acc: 1.00% | Test Loss: 0.76088, Test Acc: 0.67%\n",
            "Epoch: 34 | Loss: 0.22350, Acc: 1.00% | Test Loss: 0.70830, Test Acc: 0.67%\n",
            "Epoch: 35 | Loss: 0.22231, Acc: 1.00% | Test Loss: 0.74100, Test Acc: 0.67%\n",
            "Epoch: 36 | Loss: 0.22182, Acc: 1.00% | Test Loss: 0.60968, Test Acc: 0.67%\n",
            "Epoch: 37 | Loss: 0.22037, Acc: 1.00% | Test Loss: 0.64438, Test Acc: 0.67%\n",
            "Epoch: 38 | Loss: 0.21949, Acc: 1.00% | Test Loss: 0.70353, Test Acc: 0.67%\n",
            "Epoch: 39 | Loss: 0.21924, Acc: 1.00% | Test Loss: 0.69353, Test Acc: 0.67%\n",
            "Epoch: 40 | Loss: 0.21829, Acc: 1.00% | Test Loss: 0.72462, Test Acc: 0.68%\n",
            "Epoch: 41 | Loss: 0.21711, Acc: 1.00% | Test Loss: 0.71543, Test Acc: 0.67%\n",
            "Epoch: 42 | Loss: 0.21703, Acc: 1.00% | Test Loss: 0.65722, Test Acc: 0.67%\n",
            "Epoch: 43 | Loss: 0.21595, Acc: 1.00% | Test Loss: 0.69979, Test Acc: 0.67%\n",
            "Epoch: 44 | Loss: 0.21484, Acc: 1.00% | Test Loss: 0.70604, Test Acc: 0.67%\n",
            "Epoch: 45 | Loss: 0.21360, Acc: 1.00% | Test Loss: 0.67392, Test Acc: 0.67%\n",
            "Epoch: 46 | Loss: 0.21465, Acc: 1.00% | Test Loss: 0.67886, Test Acc: 0.67%\n",
            "Epoch: 47 | Loss: 0.21194, Acc: 1.00% | Test Loss: 0.73964, Test Acc: 0.67%\n",
            "Epoch: 48 | Loss: 0.21276, Acc: 1.00% | Test Loss: 0.80511, Test Acc: 0.67%\n",
            "Epoch: 49 | Loss: 0.21222, Acc: 1.00% | Test Loss: 0.67090, Test Acc: 0.67%\n",
            "Epoch: 50 | Loss: 0.20840, Acc: 1.00% | Test Loss: 0.61615, Test Acc: 0.67%\n",
            "early stopping happend!\n",
            "Training complete in 25m 57s\n",
            "Best val Acc: 0.677800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "previously, we said that if we train our resnet 18 model from scratch we achive better results than only linear tuning resnet 50 network, so based of that we didn't expect that this method could help model to achive better accuracy, but it did! if we see netxt part, without using data augmentation ( which we also didn't used here, because with data augmentation that we introduced when defining our data loader, teacher model accuracy would drop significantly) we achive best validation accuracy of %67, but here our best val accuracy is %68 and also we converge really faster when we use teacher model. we discuss this more in next part(part c)"
      ],
      "metadata": {
        "id": "WcEznzEd76av"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**now lets try to use 224*224 input size and see the results:**\n",
        "because it takes so much time to compute for different a and t, we select two set of a and t, first we select based on 32*32 size (a = 0.25 and t = 0.5) and second because in this case we have a better teacher model, we test a = 0.5 and t = 1 so our teacher have a bigger impact\n"
      ],
      "metadata": {
        "id": "tgMpZ0yLrjo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make teacher model untrainable\n",
        "for param in model_teacher_trained_224.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "nuWx5QItsFVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to learn based of teacher model and real labels\n",
        "def train_model_manual_Hinton_224(model,model_teacher, optimizer,scheduler,val_beark,t,a, num_epochs=25):\n",
        "  \"\"\"\n",
        "  function for train our model only based of teacher model(and not input labels) in this function we use dataloader directly. so we only can use it for our CIFAR10 dataset with our input name.\n",
        "  in this function, we resized our image to 224*224\n",
        "  inputs: \n",
        "      model: input model \n",
        "      optimizer: our optimizer(!)\n",
        "      scheduler: for changing learning rate after sum epochs\n",
        "      num_epochs: number of epoches\n",
        "      t: temperature parameter\n",
        "      a: ration between two losses (it value should be between 0 and 1)\n",
        "      val_beark: threshold for early stopping, if after \"val_beark\" steps our model don't get better, we end procces\n",
        "  output:\n",
        "      model: our trained model!\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  train_acc = []\n",
        "  train_loss = []\n",
        "  test_acc = []\n",
        "  test_loss = []\n",
        "  since = time.time()\n",
        "\n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "  best_acc = 0.0\n",
        "  best_loss = 100000000000\n",
        "  counter_val_beark = 0\n",
        "  for epoch in range(num_epochs):\n",
        "    ### Training\n",
        "    model.train()\n",
        "    loss_train = 0\n",
        "    acc_train = 0\n",
        "    counter = 1\n",
        "    for batch,label in train_data_loader_224:\n",
        "      # 1. Forward pass\n",
        "      batch = batch.to(device)\n",
        "      label = label.to(device)\n",
        "      outputs = model(batch) # model outputs raw logits \n",
        "      outputs_teacher = model_teacher(batch)\n",
        "      _, preds = torch.max(outputs, 1)\n",
        "\n",
        "      # print(y_logits)\n",
        "      # 2. Calculate loss and accuracy\n",
        "      counter = counter + 1\n",
        "      loss = Hinton_loss(outputs, outputs_teacher,label.data,t,a)\n",
        "      acc_train += torch.sum(preds == label.data)\n",
        "      loss_train = loss_train + loss\n",
        "      # 3. Optimizer zero grad\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # 4. Loss backwards\n",
        "      loss.backward()\n",
        "\n",
        "      # 5. Optimizer step\n",
        "      optimizer.step()\n",
        "    loss_train = loss_train/counter\n",
        "    acc_train = acc_train/dataset_sizes['train']\n",
        "    train_acc.append(acc_train)\n",
        "    train_loss.append(loss_train)\n",
        "    scheduler.step()\n",
        "    ### Testing\n",
        "    loss_test = 0\n",
        "    acc_test = 0\n",
        "    counter = 1\n",
        "\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "      for batch,label in test_data_loader_224:\n",
        "      # 1. Forward pass\n",
        "        batch = batch.to(device)\n",
        "        label = label.to(device)\n",
        "        output = model(batch) # model outputs raw logits \n",
        "        output_teacher = model_teacher(batch)\n",
        "        _, preds = torch.max(output, 1)\n",
        "        loss_test= Hinton_loss(output, output_teacher,label.data,t,a)\n",
        "        acc_test += torch.sum(preds == label.data)\n",
        "      loss_test = loss_test/counter\n",
        "      acc_test = acc_test/dataset_sizes['val']\n",
        "      test_loss.append(loss_test)\n",
        "      test_acc.append(acc_test)\n",
        "    if acc_test > best_acc:\n",
        "                best_acc = acc_test\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    if loss_test <= best_loss:\n",
        "                best_loss = loss_test\n",
        "                counter_val_beark = 0\n",
        "    if loss_test > best_loss:\n",
        "                counter_val_beark = counter_val_beark + 1\n",
        "                if (counter_val_beark > val_beark):\n",
        "                  print(f\"early stopping happend!\")\n",
        "                  break;\n",
        "\n",
        "    # Print out what's happening\n",
        "    if epoch % 1 == 0:\n",
        "      print(f\"Epoch: {epoch} | Loss: {loss_train:.5f}, Acc: {acc_train:.2f}% | Test Loss: {loss_test:.5f}, Test Acc: {acc_test:.2f}%\") \n",
        "  time_elapsed = time.time() - since\n",
        "  print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "  print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "  # load best model weights\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return model,best_loss,best_acc\n"
      ],
      "metadata": {
        "id": "PCHnbGgetHJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([0.1,0.25,0.5,0.75,1], dtype=torch.float64, device=device)\n",
        "t = torch.tensor([0.1,0.5,1,10], dtype=torch.float64, device=device)\n",
        "\n",
        "model_res18_Hinton_224 = models.resnet18(pretrained=False)\n",
        "num_ftrs = model_res18_Hinton_224.fc.in_features\n",
        "# Here the size of each output sample is set to 10.\n",
        "model_res18_Hinton_224.fc = nn.Linear(num_ftrs, 10)\n",
        "model_res18_Hinton_224 = model_res18_Hinton_224.to(device)\n",
        "\n",
        "print(f\"training model for a = {a[1]}, t = {t[1]}\")\n",
        "print(100*'*')\n",
        "optimizer_ft = optim.SGD(model_res18_Hinton_224.parameters(), lr=0.001, momentum=0.9)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=50, gamma=0.5)\n",
        "model_res18_Hinton_224_trained,_,_ = train_model_manual_Hinton_224(model_res18_Hinton_224,model_teacher_trained_224, optimizer_ft,exp_lr_scheduler,30,t[2],a[1],num_epochs=10) #a[1] = 0.25 and t[2] = 0.5 our selected parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79h1z95ftc6O",
        "outputId": "f36b2c1e-2f51-4229-e17c-075fd4f31c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training model for a = 0.25, t = 0.5\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 1.71489, Acc: 0.39% | Test Loss: 1.83715, Test Acc: 0.48%\n",
            "Epoch: 1 | Loss: 1.38011, Acc: 0.53% | Test Loss: 1.54563, Test Acc: 0.53%\n",
            "Epoch: 2 | Loss: 1.22728, Acc: 0.59% | Test Loss: 1.26492, Test Acc: 0.61%\n",
            "Epoch: 3 | Loss: 1.12050, Acc: 0.64% | Test Loss: 1.61467, Test Acc: 0.62%\n",
            "Epoch: 4 | Loss: 1.03290, Acc: 0.68% | Test Loss: 1.43793, Test Acc: 0.66%\n",
            "Epoch: 5 | Loss: 0.96365, Acc: 0.71% | Test Loss: 1.16515, Test Acc: 0.68%\n",
            "Epoch: 6 | Loss: 0.90841, Acc: 0.74% | Test Loss: 1.18392, Test Acc: 0.70%\n",
            "Epoch: 7 | Loss: 0.84961, Acc: 0.77% | Test Loss: 1.00066, Test Acc: 0.73%\n",
            "Epoch: 8 | Loss: 0.80225, Acc: 0.80% | Test Loss: 1.02172, Test Acc: 0.71%\n",
            "Epoch: 9 | Loss: 0.75084, Acc: 0.82% | Test Loss: 1.14093, Test Acc: 0.72%\n",
            "Training complete in 54m 26s\n",
            "Best val Acc: 0.725100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([0.1,0.25,0.5,0.75,1], dtype=torch.float64, device=device)\n",
        "t = torch.tensor([0.1,0.5,1,10], dtype=torch.float64, device=device)\n",
        "\n",
        "model_res18_Hinton_224 = models.resnet18(pretrained=False)\n",
        "num_ftrs = model_res18_Hinton_224.fc.in_features\n",
        "# Here the size of each output sample is set to 10.\n",
        "model_res18_Hinton_224.fc = nn.Linear(num_ftrs, 10)\n",
        "model_res18_Hinton_224 = model_res18_Hinton_224.to(device)\n",
        "\n",
        "print(f\"training model for a = {a[2]}, t = {t[2]}\")\n",
        "print(100*'*')\n",
        "optimizer_ft = optim.SGD(model_res18_Hinton_224.parameters(), lr=0.001, momentum=0.9)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=50, gamma=0.5)\n",
        "model_res18_Hinton_224_trained,_,_ = train_model_manual_Hinton_224(model_res18_Hinton_224,model_teacher_trained_224, optimizer_ft,exp_lr_scheduler,30,t[2],a[1],num_epochs=10) #a[1] = 0.25 and t[2] = 0.5 our selected parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Cumw2d-7VvF",
        "outputId": "311f5ccc-10b0-4393-f00a-84d8f09c1e87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training model for a = 0.5, t = 1.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 1.72415, Acc: 0.37% | Test Loss: 1.97107, Test Acc: 0.46%\n",
            "Epoch: 1 | Loss: 1.39639, Acc: 0.52% | Test Loss: 1.72417, Test Acc: 0.53%\n",
            "Epoch: 2 | Loss: 1.24315, Acc: 0.59% | Test Loss: 1.54002, Test Acc: 0.58%\n",
            "Epoch: 3 | Loss: 1.11993, Acc: 0.64% | Test Loss: 1.49448, Test Acc: 0.64%\n",
            "Epoch: 4 | Loss: 1.03052, Acc: 0.69% | Test Loss: 1.23506, Test Acc: 0.66%\n",
            "Epoch: 5 | Loss: 0.95776, Acc: 0.72% | Test Loss: 1.15089, Test Acc: 0.68%\n",
            "Epoch: 6 | Loss: 0.89605, Acc: 0.75% | Test Loss: 1.12339, Test Acc: 0.71%\n",
            "Epoch: 7 | Loss: 0.84232, Acc: 0.78% | Test Loss: 1.16909, Test Acc: 0.71%\n",
            "Epoch: 8 | Loss: 0.78843, Acc: 0.80% | Test Loss: 1.34211, Test Acc: 0.72%\n",
            "Epoch: 9 | Loss: 0.74605, Acc: 0.83% | Test Loss: 1.23350, Test Acc: 0.75%\n",
            "Training complete in 54m 18s\n",
            "Best val Acc: 0.745200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "it is important to note that, if we train for more batches, we probably get better results for our train accuracy, but it takes so long time to compute ( each 10 batch when input size = (224*224) takes around 1 hour!)\n",
        "but lets train in for more epoches to prove our statemetn, but in general for saving time (and resources!) we only train for 10 epoches and compare them that way( so it is fair)"
      ],
      "metadata": {
        "id": "3R2Hy_pNXCYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([0.1,0.25,0.5,0.75,1], dtype=torch.float64, device=device)\n",
        "t = torch.tensor([0.1,0.5,1,10], dtype=torch.float64, device=device)\n",
        "\n",
        "model_res18_Hinton_224 = models.resnet18(pretrained=False)\n",
        "num_ftrs = model_res18_Hinton_224.fc.in_features\n",
        "# Here the size of each output sample is set to 10.\n",
        "model_res18_Hinton_224.fc = nn.Linear(num_ftrs, 10)\n",
        "model_res18_Hinton_224 = model_res18_Hinton_224.to(device)\n",
        "\n",
        "print(f\"training model for a = {a[2]}, t = {t[2]}\")\n",
        "print(100*'*')\n",
        "optimizer_ft = optim.SGD(model_res18_Hinton_224.parameters(), lr=0.001, momentum=0.9)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=50, gamma=0.5)\n",
        "model_res18_Hinton_224_trained,_,_ = train_model_manual_Hinton_224(model_res18_Hinton_224,model_teacher_trained_224, optimizer_ft,exp_lr_scheduler,30,t[2],a[1],num_epochs=20) #a[1] = 0.25 and t[2] = 0.5 our selected parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrVsHVRRXh0T",
        "outputId": "6c6b83dd-92be-4d3c-cfa4-13971430fd5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training model for a = 0.5, t = 1.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 1.72342, Acc: 0.37% | Test Loss: 1.57112, Test Acc: 0.45%\n",
            "Epoch: 1 | Loss: 1.38558, Acc: 0.52% | Test Loss: 1.61890, Test Acc: 0.55%\n",
            "Epoch: 2 | Loss: 1.22674, Acc: 0.59% | Test Loss: 1.68562, Test Acc: 0.58%\n",
            "Epoch: 3 | Loss: 1.12206, Acc: 0.64% | Test Loss: 1.30458, Test Acc: 0.63%\n",
            "Epoch: 4 | Loss: 1.03530, Acc: 0.68% | Test Loss: 1.16150, Test Acc: 0.67%\n",
            "Epoch: 5 | Loss: 0.96591, Acc: 0.72% | Test Loss: 1.22491, Test Acc: 0.67%\n",
            "Epoch: 6 | Loss: 0.91272, Acc: 0.74% | Test Loss: 1.19324, Test Acc: 0.71%\n",
            "Epoch: 7 | Loss: 0.85775, Acc: 0.77% | Test Loss: 0.91925, Test Acc: 0.71%\n",
            "Epoch: 8 | Loss: 0.81375, Acc: 0.79% | Test Loss: 1.04182, Test Acc: 0.73%\n",
            "Epoch: 9 | Loss: 0.76874, Acc: 0.81% | Test Loss: 1.04186, Test Acc: 0.71%\n",
            "Epoch: 10 | Loss: 0.72123, Acc: 0.84% | Test Loss: 1.12301, Test Acc: 0.75%\n",
            "Epoch: 11 | Loss: 0.68064, Acc: 0.86% | Test Loss: 1.01378, Test Acc: 0.74%\n",
            "Epoch: 12 | Loss: 0.64266, Acc: 0.88% | Test Loss: 1.05732, Test Acc: 0.73%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "as we can see, our best test accuracy is still around 75%, but our train accuracy gets better by each epoch.\n"
      ],
      "metadata": {
        "id": "0T1P64app3Xg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "once again, in part C we first train model when input size is 32*32 and compare it with result of part B ( with same input size) and then, we train model when input size is 224*224 and compare results again (with model with same input size)"
      ],
      "metadata": {
        "id": "SaiCmzsZq1pY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part C)**\n",
        "Train Resnet 18 model from scratch, we test it two method, with and without data augmentation\n",
        "first with data augmentation"
      ],
      "metadata": {
        "id": "AFHuBN-o76ko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_res18_scratch = models.resnet18(pretrained=False)\n",
        "num_ftrs = model_res18_scratch.fc.in_features\n",
        "# Here the size of each output sample is set to 10.\n",
        "model_res18_scratch.fc = nn.Linear(num_ftrs, 10)\n",
        "\n",
        "model_res18_scratch = model_res18_scratch.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_res18_scratch.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.5 every 20 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=50, gamma=0.5)\n",
        "model_res18_scratch,best_loss,best_acc = train_model_manual_augment(model_res18_scratch, criterion, optimizer_ft,exp_lr_scheduler,30,num_epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4FsK8fi1OLh",
        "outputId": "f59937f4-700f-417c-dfa1-53650ac5adf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 1.73562, Acc: 0.37% | Test Loss: 1.42280, Test Acc: 0.47%\n",
            "Epoch: 1 | Loss: 1.45202, Acc: 0.47% | Test Loss: 1.12534, Test Acc: 0.54%\n",
            "Epoch: 2 | Loss: 1.32268, Acc: 0.52% | Test Loss: 0.96853, Test Acc: 0.56%\n",
            "Epoch: 3 | Loss: 1.22944, Acc: 0.56% | Test Loss: 0.85799, Test Acc: 0.59%\n",
            "Epoch: 4 | Loss: 1.16285, Acc: 0.58% | Test Loss: 0.84760, Test Acc: 0.63%\n",
            "Epoch: 5 | Loss: 1.10393, Acc: 0.61% | Test Loss: 1.03049, Test Acc: 0.64%\n",
            "Epoch: 6 | Loss: 1.05250, Acc: 0.62% | Test Loss: 1.01132, Test Acc: 0.65%\n",
            "Epoch: 7 | Loss: 1.00910, Acc: 0.64% | Test Loss: 0.56364, Test Acc: 0.67%\n",
            "Epoch: 8 | Loss: 0.97647, Acc: 0.65% | Test Loss: 0.71269, Test Acc: 0.68%\n",
            "Epoch: 9 | Loss: 0.94069, Acc: 0.67% | Test Loss: 0.75178, Test Acc: 0.69%\n",
            "Epoch: 10 | Loss: 0.90847, Acc: 0.68% | Test Loss: 0.46894, Test Acc: 0.70%\n",
            "Epoch: 11 | Loss: 0.87508, Acc: 0.69% | Test Loss: 0.57663, Test Acc: 0.69%\n",
            "Epoch: 12 | Loss: 0.85800, Acc: 0.70% | Test Loss: 0.65119, Test Acc: 0.71%\n",
            "Epoch: 13 | Loss: 0.83520, Acc: 0.70% | Test Loss: 0.53332, Test Acc: 0.72%\n",
            "Epoch: 14 | Loss: 0.80942, Acc: 0.71% | Test Loss: 0.59062, Test Acc: 0.72%\n",
            "Epoch: 15 | Loss: 0.78625, Acc: 0.72% | Test Loss: 0.66183, Test Acc: 0.72%\n",
            "Epoch: 16 | Loss: 0.76791, Acc: 0.73% | Test Loss: 0.40652, Test Acc: 0.73%\n",
            "Epoch: 17 | Loss: 0.74588, Acc: 0.74% | Test Loss: 0.56887, Test Acc: 0.73%\n",
            "Epoch: 18 | Loss: 0.72805, Acc: 0.74% | Test Loss: 0.57641, Test Acc: 0.73%\n",
            "Epoch: 19 | Loss: 0.70462, Acc: 0.75% | Test Loss: 0.61663, Test Acc: 0.73%\n",
            "Epoch: 20 | Loss: 0.63780, Acc: 0.78% | Test Loss: 0.48206, Test Acc: 0.75%\n",
            "Epoch: 21 | Loss: 0.61248, Acc: 0.78% | Test Loss: 0.44973, Test Acc: 0.75%\n",
            "Epoch: 22 | Loss: 0.60232, Acc: 0.79% | Test Loss: 0.52682, Test Acc: 0.75%\n",
            "Epoch: 23 | Loss: 0.59524, Acc: 0.79% | Test Loss: 0.47332, Test Acc: 0.76%\n",
            "Epoch: 24 | Loss: 0.59349, Acc: 0.79% | Test Loss: 0.48901, Test Acc: 0.75%\n",
            "Epoch: 25 | Loss: 0.58950, Acc: 0.79% | Test Loss: 0.46952, Test Acc: 0.75%\n",
            "Epoch: 26 | Loss: 0.58297, Acc: 0.79% | Test Loss: 0.48275, Test Acc: 0.76%\n",
            "early stopping happend!\n",
            "Training complete in 21m 20s\n",
            "Best val Acc: 0.758900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "now without data augmentation.\n",
        "\n",
        "**Important note:**\n",
        "as estated before, because just linear-tuning resnet 50 with data which have augmentation, lower our accuracy (form 54 to 45) so we trained part B model only using original data ( without augmentation) so we have to compare results of part B with result of model below:\n"
      ],
      "metadata": {
        "id": "W5FRlBW4P8fG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_res18_scratch = models.resnet18(pretrained=False)\n",
        "num_ftrs = model_res18_scratch.fc.in_features\n",
        "# Here the size of each output sample is set to 10.\n",
        "model_res18_scratch.fc = nn.Linear(num_ftrs, 10)\n",
        "\n",
        "model_res18_scratch = model_res18_scratch.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_res18_scratch.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.5 every 20 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=20, gamma=0.5)\n",
        "model_res18_scratch_without_aug,best_loss,best_acc = train_model_manual(model_res18_scratch, criterion, optimizer_ft,exp_lr_scheduler,10,num_epochs=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0y8eehiSQSNz",
        "outputId": "09e6295a-5d67-4b9f-f3c9-dcc0bb7ccc6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 1.58789, Acc: 0.42% | Test Loss: 1.12895, Test Acc: 0.51%\n",
            "Epoch: 1 | Loss: 1.24391, Acc: 0.55% | Test Loss: 0.67372, Test Acc: 0.56%\n",
            "Epoch: 2 | Loss: 1.06233, Acc: 0.62% | Test Loss: 0.97934, Test Acc: 0.60%\n",
            "Epoch: 3 | Loss: 0.89989, Acc: 0.68% | Test Loss: 0.72847, Test Acc: 0.63%\n",
            "Epoch: 4 | Loss: 0.76523, Acc: 0.73% | Test Loss: 0.70989, Test Acc: 0.63%\n",
            "Epoch: 5 | Loss: 0.64312, Acc: 0.77% | Test Loss: 0.79038, Test Acc: 0.63%\n",
            "Epoch: 6 | Loss: 0.52558, Acc: 0.82% | Test Loss: 0.46035, Test Acc: 0.64%\n",
            "Epoch: 7 | Loss: 0.42547, Acc: 0.85% | Test Loss: 1.05604, Test Acc: 0.64%\n",
            "Epoch: 8 | Loss: 0.33972, Acc: 0.88% | Test Loss: 0.84740, Test Acc: 0.64%\n",
            "Epoch: 9 | Loss: 0.28088, Acc: 0.90% | Test Loss: 0.42529, Test Acc: 0.64%\n",
            "Epoch: 10 | Loss: 0.22531, Acc: 0.92% | Test Loss: 1.54129, Test Acc: 0.65%\n",
            "Epoch: 11 | Loss: 0.18718, Acc: 0.93% | Test Loss: 1.54796, Test Acc: 0.66%\n",
            "Epoch: 12 | Loss: 0.15597, Acc: 0.94% | Test Loss: 1.30661, Test Acc: 0.65%\n",
            "Epoch: 13 | Loss: 0.14276, Acc: 0.95% | Test Loss: 1.03229, Test Acc: 0.65%\n",
            "Epoch: 14 | Loss: 0.11443, Acc: 0.96% | Test Loss: 1.18010, Test Acc: 0.66%\n",
            "Epoch: 15 | Loss: 0.09651, Acc: 0.97% | Test Loss: 1.14501, Test Acc: 0.66%\n",
            "Epoch: 16 | Loss: 0.08629, Acc: 0.97% | Test Loss: 0.80742, Test Acc: 0.66%\n",
            "Epoch: 17 | Loss: 0.09289, Acc: 0.97% | Test Loss: 1.12239, Test Acc: 0.65%\n",
            "Epoch: 19 | Loss: 0.06700, Acc: 0.98% | Test Loss: 0.91153, Test Acc: 0.67%\n",
            "early stopping happend!\n",
            "Training complete in 7m 33s\n",
            "Best val Acc: 0.674400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "as we estated befor,we couldn't use data augmentation in part B because our teacher model didn't trained on a data with augmentation ( if we add data augmentation, our accuracy would drop from 56% to 45%! and because of that we didn't use data augmentation in part B either) when we have data augmentation we could achive bettter validation accuracy (but train accuracy would drop) \n",
        "if we compare part B and C on data that don't have data augmentation, results are very close, because when we train model from scratch we achive better result than linear tuning, but we can see that using teacher model and Knowledge Distillation method can really help us, if we compare rate of convergence of part B and C, part B converge much better and it loss value has a better pattern ( we can't really compare loss function of theese two parts directly because they have different critation, but generally speaking we can see a better decreasing pattern ) and it is because of distribution of logits, which have high level information in them and can help model to converge to a better result ( 68 compare to 67) and faster."
      ],
      "metadata": {
        "id": "yWNLVggyIbzj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "now testing for 224*224 input size"
      ],
      "metadata": {
        "id": "lpkGN25IcLFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_res18_scratch = models.resnet18(pretrained=False)\n",
        "num_ftrs = model_res18_scratch.fc.in_features\n",
        "# Here the size of each output sample is set to 10.\n",
        "model_res18_scratch.fc = nn.Linear(num_ftrs, 10)\n",
        "\n",
        "model_res18_scratch = model_res18_scratch.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_res18_scratch.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.5 every 20 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=50, gamma=0.1)\n",
        "model_res18_scratch_without_aug,best_loss,best_acc = train_model_manual_224(model_res18_scratch, criterion, optimizer_ft,exp_lr_scheduler,5,num_epochs=10)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZnHaZ6FcPmR",
        "outputId": "76740656-6766-47cb-e72c-21379abaa07d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 1.68219, Acc: 0.38% | Test Loss: 1.48208, Test Acc: 0.46%\n",
            "Epoch: 1 | Loss: 1.29863, Acc: 0.53% | Test Loss: 1.30886, Test Acc: 0.56%\n",
            "Epoch: 2 | Loss: 1.10419, Acc: 0.60% | Test Loss: 1.16750, Test Acc: 0.57%\n",
            "Epoch: 3 | Loss: 0.95679, Acc: 0.66% | Test Loss: 1.19588, Test Acc: 0.61%\n",
            "Epoch: 4 | Loss: 0.79871, Acc: 0.72% | Test Loss: 0.98815, Test Acc: 0.70%\n",
            "Epoch: 5 | Loss: 0.76838, Acc: 0.73% | Test Loss: 0.88233, Test Acc: 0.70%\n",
            "Epoch: 6 | Loss: 0.74659, Acc: 0.74% | Test Loss: 0.88661, Test Acc: 0.70%\n",
            "Epoch: 7 | Loss: 0.72673, Acc: 0.75% | Test Loss: 0.79384, Test Acc: 0.71%\n",
            "Epoch: 8 | Loss: 0.70703, Acc: 0.75% | Test Loss: 0.85173, Test Acc: 0.71%\n",
            "Epoch: 9 | Loss: 0.70235, Acc: 0.76% | Test Loss: 0.82246, Test Acc: 0.71%\n",
            "Training complete in 26m 12s\n",
            "Best val Acc: 0.709400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "so if we want to compare results to part B, best model was trained when we choose a = 0.5 and t = 1, so teacher model had a significant role in training the model. in that case our best val accuracy was %75 which is higher than 71 so it nearly is 4 percent increase, but in this case convergance rate didn't get better that much when we use teacher model, nonetheless, we can see when input size is 224*224 our teacher model help us to achive better accuracy ( it's partially because linear-tuning resnet 50 when input size is the same as imagenet, give us a really good results so we have really good teacher model "
      ],
      "metadata": {
        "id": "IRdf7xj-cNUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Part D**)Fine tuning all parts of model and see the results\n",
        "again for this part, we use two methods, in first method we don't use data augmentation and in scond method we use and see difference"
      ],
      "metadata": {
        "id": "tB14BsfN1OW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft = models.resnet50(pretrained=True)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "model_ft.fc = nn.Linear(num_ftrs, 10)\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.5 every 50 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=50, gamma=0.5)\n",
        "model_ft,_,_ = train_model_manual(model_ft, criterion, optimizer_ft,exp_lr_scheduler,20,\n",
        "                         num_epochs=100)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXxpBO-n1UK6",
        "outputId": "0a45390d-ce7c-4246-c75d-e424e81d71e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 0.96082, Acc: 0.67% | Test Loss: 0.67319, Test Acc: 0.78%\n",
            "Epoch: 1 | Loss: 0.50139, Acc: 0.83% | Test Loss: 0.43050, Test Acc: 0.81%\n",
            "Epoch: 2 | Loss: 0.33285, Acc: 0.89% | Test Loss: 0.80661, Test Acc: 0.83%\n",
            "Epoch: 3 | Loss: 0.22147, Acc: 0.92% | Test Loss: 0.42996, Test Acc: 0.83%\n",
            "Epoch: 4 | Loss: 0.16164, Acc: 0.94% | Test Loss: 0.62235, Test Acc: 0.83%\n",
            "Epoch: 5 | Loss: 0.11759, Acc: 0.96% | Test Loss: 0.95408, Test Acc: 0.83%\n",
            "Epoch: 6 | Loss: 0.08741, Acc: 0.97% | Test Loss: 1.40490, Test Acc: 0.83%\n",
            "Epoch: 7 | Loss: 0.07778, Acc: 0.97% | Test Loss: 1.06363, Test Acc: 0.83%\n",
            "Epoch: 8 | Loss: 0.05757, Acc: 0.98% | Test Loss: 0.88969, Test Acc: 0.84%\n",
            "Epoch: 9 | Loss: 0.05572, Acc: 0.98% | Test Loss: 1.07528, Test Acc: 0.84%\n",
            "Epoch: 10 | Loss: 0.04601, Acc: 0.98% | Test Loss: 1.16554, Test Acc: 0.84%\n",
            "Epoch: 11 | Loss: 0.04387, Acc: 0.98% | Test Loss: 1.05286, Test Acc: 0.84%\n",
            "Epoch: 12 | Loss: 0.02997, Acc: 0.99% | Test Loss: 1.36141, Test Acc: 0.84%\n",
            "Epoch: 13 | Loss: 0.02807, Acc: 0.99% | Test Loss: 0.66241, Test Acc: 0.84%\n",
            "Epoch: 14 | Loss: 0.02587, Acc: 0.99% | Test Loss: 1.22419, Test Acc: 0.84%\n",
            "Epoch: 15 | Loss: 0.02025, Acc: 0.99% | Test Loss: 1.09094, Test Acc: 0.84%\n",
            "Epoch: 16 | Loss: 0.01881, Acc: 0.99% | Test Loss: 1.30255, Test Acc: 0.84%\n",
            "Epoch: 17 | Loss: 0.02025, Acc: 0.99% | Test Loss: 1.04165, Test Acc: 0.84%\n",
            "Epoch: 18 | Loss: 0.01732, Acc: 0.99% | Test Loss: 1.18281, Test Acc: 0.85%\n",
            "Epoch: 19 | Loss: 0.01764, Acc: 0.99% | Test Loss: 1.31206, Test Acc: 0.84%\n",
            "Epoch: 20 | Loss: 0.01436, Acc: 1.00% | Test Loss: 1.01147, Test Acc: 0.84%\n",
            "Epoch: 21 | Loss: 0.01294, Acc: 1.00% | Test Loss: 0.64586, Test Acc: 0.84%\n",
            "Epoch: 22 | Loss: 0.01702, Acc: 0.99% | Test Loss: 1.90209, Test Acc: 0.84%\n",
            "Epoch: 23 | Loss: 0.01109, Acc: 1.00% | Test Loss: 1.57289, Test Acc: 0.84%\n",
            "early stopping happend!\n",
            "Training complete in 15m 14s\n",
            "Best val Acc: 0.845100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "now lets train whole resnet50 model (with imagenet weights) when we added data augmentation"
      ],
      "metadata": {
        "id": "upJuYhxTNNtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft = models.resnet50(pretrained=True)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "model_ft.fc = nn.Linear(num_ftrs, 10)\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.5 every 50 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=50, gamma=0.5)\n",
        "model_ft,_,_ = train_model_manual_augment(model_ft, criterion, optimizer_ft,exp_lr_scheduler,20,\n",
        "                         num_epochs=100)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMELpczpNUZN",
        "outputId": "51a86af4-96d0-4cd7-fbbd-cf3e5ac4a9b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 1.21170, Acc: 0.57% | Test Loss: 0.76899, Test Acc: 0.73%\n",
            "Epoch: 1 | Loss: 0.80030, Acc: 0.72% | Test Loss: 0.42701, Test Acc: 0.80%\n",
            "Epoch: 2 | Loss: 0.69075, Acc: 0.76% | Test Loss: 0.33888, Test Acc: 0.81%\n",
            "Epoch: 3 | Loss: 0.61725, Acc: 0.78% | Test Loss: 0.46636, Test Acc: 0.82%\n",
            "Epoch: 4 | Loss: 0.55869, Acc: 0.80% | Test Loss: 0.25251, Test Acc: 0.84%\n",
            "Epoch: 5 | Loss: 0.52015, Acc: 0.82% | Test Loss: 0.33335, Test Acc: 0.84%\n",
            "Epoch: 6 | Loss: 0.48399, Acc: 0.83% | Test Loss: 0.21037, Test Acc: 0.85%\n",
            "Epoch: 7 | Loss: 0.45322, Acc: 0.84% | Test Loss: 0.44566, Test Acc: 0.85%\n",
            "Epoch: 8 | Loss: 0.42699, Acc: 0.85% | Test Loss: 0.38468, Test Acc: 0.85%\n",
            "Epoch: 9 | Loss: 0.40764, Acc: 0.86% | Test Loss: 0.29940, Test Acc: 0.86%\n",
            "Epoch: 10 | Loss: 0.38636, Acc: 0.86% | Test Loss: 0.36554, Test Acc: 0.86%\n",
            "Epoch: 11 | Loss: 0.36501, Acc: 0.87% | Test Loss: 0.35551, Test Acc: 0.86%\n",
            "Epoch: 12 | Loss: 0.34538, Acc: 0.88% | Test Loss: 0.36042, Test Acc: 0.86%\n",
            "Epoch: 13 | Loss: 0.32478, Acc: 0.89% | Test Loss: 0.36519, Test Acc: 0.86%\n",
            "Epoch: 14 | Loss: 0.31155, Acc: 0.89% | Test Loss: 0.32863, Test Acc: 0.86%\n",
            "Epoch: 15 | Loss: 0.29835, Acc: 0.89% | Test Loss: 0.37606, Test Acc: 0.86%\n",
            "Epoch: 16 | Loss: 0.28153, Acc: 0.90% | Test Loss: 0.29572, Test Acc: 0.86%\n",
            "Epoch: 17 | Loss: 0.27010, Acc: 0.91% | Test Loss: 0.46963, Test Acc: 0.86%\n",
            "Epoch: 18 | Loss: 0.25521, Acc: 0.91% | Test Loss: 0.32287, Test Acc: 0.86%\n",
            "Epoch: 19 | Loss: 0.24990, Acc: 0.91% | Test Loss: 0.38497, Test Acc: 0.86%\n",
            "Epoch: 20 | Loss: 0.23813, Acc: 0.92% | Test Loss: 0.39855, Test Acc: 0.86%\n",
            "Epoch: 21 | Loss: 0.22548, Acc: 0.92% | Test Loss: 0.25288, Test Acc: 0.87%\n",
            "Epoch: 22 | Loss: 0.21893, Acc: 0.92% | Test Loss: 0.34603, Test Acc: 0.87%\n",
            "Epoch: 23 | Loss: 0.21094, Acc: 0.93% | Test Loss: 0.31083, Test Acc: 0.86%\n",
            "Epoch: 24 | Loss: 0.20232, Acc: 0.93% | Test Loss: 0.74600, Test Acc: 0.86%\n",
            "Epoch: 25 | Loss: 0.19656, Acc: 0.93% | Test Loss: 0.70141, Test Acc: 0.87%\n",
            "Epoch: 26 | Loss: 0.18104, Acc: 0.94% | Test Loss: 0.48670, Test Acc: 0.86%\n",
            "early stopping happend!\n",
            "Training complete in 27m 31s\n",
            "Best val Acc: 0.867700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model_ft.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "bRtiqOqJiMIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "as we can see when we use data augmentation we can achive better test result. when we fine tune whole network, as we can see, results are much better ( 87% compared to %76 when we use data augmentation and 84 compared to 67 when we are not) it is beacuse when we train all network, we have these advantages:\n",
        "\n",
        "\n",
        "1.   we have more parameters to train so our model is more complex and can learn data very well (it has drop out to prevent overfitting)\n",
        "2.   fine tuning whole network is like to train a model from a good initialization, so it guarantees that model converge to a good answer.\n",
        "\n",
        "also somet other important thing that is important to note is that cifar dataset have 32 * 32 size, but imagenet input size is 224 * 224, in pytorch it is handeld by using adaptive pooling so having smaller size don't affect trainability of model, but if we want to just use pretrained network without fine-tuning all parts it doesn't work very well.\n",
        "\n",
        "so in the end, it seems wise to fine tune whole model when new dataset have similar distribution ( both colored images of different objects) and we have sufficent data and usully it will have a far better result."
      ],
      "metadata": {
        "id": "XvDUuscggXFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "now, lets try fine-tuning whole network when input size is 224 * 224"
      ],
      "metadata": {
        "id": "2edHScarjlrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft = models.resnet50(pretrained=True)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "model_ft.fc = nn.Linear(num_ftrs, 10)\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=50, gamma=0.5)\n",
        "model_ft,_,_ = train_model_manual_224(model_ft, criterion, optimizer_ft,exp_lr_scheduler,20,\n",
        "                         num_epochs=5)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fof9AJhj-qG",
        "outputId": "78b655a3-dca6-411a-980c-2aea6a56c0ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 0.38784, Acc: 0.88% | Test Loss: 0.27952, Test Acc: 0.95%\n",
            "Epoch: 1 | Loss: 0.10350, Acc: 0.97% | Test Loss: 0.21202, Test Acc: 0.95%\n",
            "Epoch: 2 | Loss: 0.04650, Acc: 0.99% | Test Loss: 0.36130, Test Acc: 0.96%\n",
            "Epoch: 3 | Loss: 0.02339, Acc: 0.99% | Test Loss: 0.22868, Test Acc: 0.96%\n",
            "Epoch: 4 | Loss: 0.01302, Acc: 1.00% | Test Loss: 0.20545, Test Acc: 0.96%\n",
            "Training complete in 41m 43s\n",
            "Best val Acc: 0.962200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "once again, we see that fine-tuning whole network increases accuracy. when input size is 224 * 224, linear-tuning gets a better result (compared to when we use 32 * 32 input) because information extracted in the end is exactly what networks extract when inputs are from imagenet, and cifar classes are in imagenet, but imagenet have to learn more classes so it's more genral, but when we fine-tune whole network, we train all weights to just classify this 10 classes ( making it more capable of predicting just this classes) so it's improve our accuracy.\n",
        "when we upscale images to 224 * 224, we have more samples to learn from ( each multipication plays a roll on updating gradients, so it is like we train network more) and in the end we get a far better results that other network (it nearlly works flawlessly)"
      ],
      "metadata": {
        "id": "hI0BUHZ2uP8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part D extra:**\n",
        "\n",
        "for when input size is 32 * 32, lets use network that we trained on part D ( with using data augmentation) as a teacher model and use hinton loss and see it effects (just like part C but with a better model)"
      ],
      "metadata": {
        "id": "GNu0H3m5gyrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to learn just based of teacher model and real labels \n",
        "# in this function we use data with augmentation\n",
        "def train_model_manual_Hinton_augment(model,model_teacher, optimizer,scheduler,val_beark,t,a, num_epochs=25):\n",
        "  \"\"\"\n",
        "  function for train our model only based of teacher model(and not input labels) in this function we use dataloader directly. so we only can use it for our CIFAR10 dataset with our input name.\n",
        "  inputs: \n",
        "      model: input model \n",
        "      optimizer: our optimizer(!)\n",
        "      scheduler: for changing learning rate after sum epochs\n",
        "      num_epochs: number of epoches\n",
        "      t: temperature parameter\n",
        "      a: ration between two losses (it value should be between 0 and 1)\n",
        "      val_beark: threshold for early stopping, if after \"val_beark\" steps our model don't get better, we end procces\n",
        "  output:\n",
        "      model: our trained model!\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  train_acc = []\n",
        "  train_loss = []\n",
        "  test_acc = []\n",
        "  test_loss = []\n",
        "  since = time.time()\n",
        "\n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "  best_acc = 0.0\n",
        "  best_loss = 100000000000\n",
        "  counter_val_beark = 0\n",
        "  for epoch in range(num_epochs):\n",
        "    ### Training\n",
        "    model.train()\n",
        "    loss_train = 0\n",
        "    acc_train = 0\n",
        "    counter = 1\n",
        "    for batch,label in train_data_loader_aug:\n",
        "      # 1. Forward pass\n",
        "      batch = batch.to(device)\n",
        "      label = label.to(device)\n",
        "      outputs = model(batch) # model outputs raw logits \n",
        "      outputs_teacher = model_teacher(batch)\n",
        "      _, preds = torch.max(outputs, 1)\n",
        "\n",
        "      # print(y_logits)\n",
        "      # 2. Calculate loss and accuracy\n",
        "      counter = counter + 1\n",
        "      loss = Hinton_loss(outputs, outputs_teacher,label.data,t,a)\n",
        "      acc_train += torch.sum(preds == label.data)\n",
        "      loss_train = loss_train + loss\n",
        "      # 3. Optimizer zero grad\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # 4. Loss backwards\n",
        "      loss.backward()\n",
        "\n",
        "      # 5. Optimizer step\n",
        "      optimizer.step()\n",
        "    loss_train = loss_train/counter\n",
        "    acc_train = acc_train/dataset_sizes['train']\n",
        "    train_acc.append(acc_train)\n",
        "    train_loss.append(loss_train)\n",
        "    scheduler.step()\n",
        "    ### Testing\n",
        "    loss_test = 0\n",
        "    acc_test = 0\n",
        "    counter = 1\n",
        "\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "      for batch,label in test_data_loader:\n",
        "      # 1. Forward pass\n",
        "        batch = batch.to(device)\n",
        "        label = label.to(device)\n",
        "        output = model(batch) # model outputs raw logits \n",
        "        output_teacher = model_teacher(batch)\n",
        "        _, preds = torch.max(output, 1)\n",
        "        loss_test= Hinton_loss(output, output_teacher,label.data,t,a)\n",
        "        acc_test += torch.sum(preds == label.data)\n",
        "      loss_test = loss_test/counter\n",
        "      acc_test = acc_test/dataset_sizes['val']\n",
        "      test_loss.append(loss_test)\n",
        "      test_acc.append(acc_test)\n",
        "    if acc_test > best_acc:\n",
        "                best_acc = acc_test\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    if loss_test <= best_loss:\n",
        "                best_loss = loss_test\n",
        "                counter_val_beark = 0\n",
        "    if loss_test > best_loss:\n",
        "                counter_val_beark = counter_val_beark + 1\n",
        "                if (counter_val_beark > val_beark):\n",
        "                  print(f\"early stopping happend!\")\n",
        "                  break;\n",
        "\n",
        "    # Print out what's happening\n",
        "    if epoch % 1 == 0:\n",
        "      print(f\"Epoch: {epoch} | Loss: {loss_train:.5f}, Acc: {acc_train:.2f}% | Test Loss: {loss_test:.5f}, Test Acc: {acc_test:.2f}%\") \n",
        "  time_elapsed = time.time() - since\n",
        "  print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "  print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "  # load best model weights\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return model,best_loss,best_acc\n"
      ],
      "metadata": {
        "id": "4LRbyKP-YbUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([0.1,0.25,0.5,0.75,1], dtype=torch.float64, device=device)\n",
        "t = torch.tensor([0.01,0.1,0.5,1,10,100], dtype=torch.float64, device=device)\n",
        "score_mat = np.zeros((len(a),len(t)))\n",
        "for i in range(len(a)):\n",
        "  for j in range(len(t)):\n",
        "    model_res18_Hinton = models.resnet18(pretrained=False)\n",
        "    num_ftrs = model_res18_Hinton.fc.in_features\n",
        "    # Here the size of each output sample is set to 10.\n",
        "    model_res18_Hinton.fc = nn.Linear(num_ftrs, 10)\n",
        "    model_res18_Hinton = model_res18_Hinton.to(device)\n",
        "\n",
        "    print(f\"training model for a = {a[i]}, t = {t[j]}\")\n",
        "    print(100*'*')\n",
        "    optimizer_ft = optim.SGD(model_res18_Hinton.parameters(), lr=0.001, momentum=0.9)\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=50, gamma=0.5)\n",
        "    _,_,score_mat[i,j] = train_model_manual_Hinton_augment(model_res18_Hinton,model_ft, optimizer_ft,exp_lr_scheduler,30,t[j],a[i],num_epochs=3)\n",
        "    print(100*'*')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wve8TI5hn6X",
        "outputId": "d26fec61-3366-468d-e960-1a6fd09ed2f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training model for a = 0.1, t = 0.01\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 1.57749, Acc: 0.36% | Test Loss: 1.04100, Test Acc: 0.47%\n",
            "Epoch: 1 | Loss: 1.31459, Acc: 0.47% | Test Loss: 0.80627, Test Acc: 0.53%\n",
            "Epoch: 2 | Loss: 1.20866, Acc: 0.51% | Test Loss: 0.78175, Test Acc: 0.56%\n",
            "Training complete in 2m 53s\n",
            "Best val Acc: 0.557900\n",
            "****************************************************************************************************\n",
            "training model for a = 0.1, t = 0.1\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 1.58037, Acc: 0.36% | Test Loss: 0.99646, Test Acc: 0.46%\n",
            "Epoch: 1 | Loss: 1.31376, Acc: 0.47% | Test Loss: 0.85720, Test Acc: 0.53%\n",
            "Epoch: 2 | Loss: 1.20091, Acc: 0.52% | Test Loss: 0.69388, Test Acc: 0.58%\n",
            "Training complete in 2m 44s\n",
            "Best val Acc: 0.576800\n",
            "****************************************************************************************************\n",
            "training model for a = 0.1, t = 0.5\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 1.60130, Acc: 0.37% | Test Loss: 1.04040, Test Acc: 0.49%\n",
            "Epoch: 1 | Loss: 1.32893, Acc: 0.48% | Test Loss: 1.06747, Test Acc: 0.53%\n",
            "Epoch: 2 | Loss: 1.22155, Acc: 0.53% | Test Loss: 0.81241, Test Acc: 0.58%\n",
            "Training complete in 2m 44s\n",
            "Best val Acc: 0.581300\n",
            "****************************************************************************************************\n",
            "training model for a = 0.1, t = 1.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 1.72385, Acc: 0.37% | Test Loss: 1.31615, Test Acc: 0.48%\n",
            "Epoch: 1 | Loss: 1.44475, Acc: 0.48% | Test Loss: 0.95897, Test Acc: 0.53%\n",
            "Epoch: 2 | Loss: 1.32922, Acc: 0.53% | Test Loss: 0.83221, Test Acc: 0.57%\n",
            "Training complete in 2m 41s\n",
            "Best val Acc: 0.574100\n",
            "****************************************************************************************************\n",
            "training model for a = 0.1, t = 10.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 20.66618, Acc: 0.33% | Test Loss: 24.09371, Test Acc: 0.43%\n",
            "Epoch: 1 | Loss: 17.33912, Acc: 0.45% | Test Loss: 19.57917, Test Acc: 0.48%\n",
            "Epoch: 2 | Loss: 16.11200, Acc: 0.50% | Test Loss: 23.83499, Test Acc: 0.53%\n",
            "Training complete in 2m 41s\n",
            "Best val Acc: 0.526800\n",
            "****************************************************************************************************\n",
            "training model for a = 0.1, t = 100.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 1 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 2 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Training complete in 2m 42s\n",
            "Best val Acc: 0.100000\n",
            "****************************************************************************************************\n",
            "training model for a = 0.25, t = 0.01\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 1.31683, Acc: 0.36% | Test Loss: 0.96873, Test Acc: 0.48%\n",
            "Epoch: 1 | Loss: 1.11073, Acc: 0.46% | Test Loss: 0.85209, Test Acc: 0.51%\n",
            "Epoch: 2 | Loss: 1.02270, Acc: 0.51% | Test Loss: 0.75456, Test Acc: 0.55%\n",
            "Training complete in 2m 40s\n",
            "Best val Acc: 0.545400\n",
            "****************************************************************************************************\n",
            "training model for a = 0.25, t = 0.1\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 1.31634, Acc: 0.36% | Test Loss: 0.82647, Test Acc: 0.46%\n",
            "Epoch: 1 | Loss: 1.10411, Acc: 0.47% | Test Loss: 0.75896, Test Acc: 0.52%\n",
            "Epoch: 2 | Loss: 1.02207, Acc: 0.51% | Test Loss: 0.89658, Test Acc: 0.56%\n",
            "Training complete in 2m 42s\n",
            "Best val Acc: 0.556800\n",
            "****************************************************************************************************\n",
            "training model for a = 0.25, t = 0.5\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 1.43677, Acc: 0.35% | Test Loss: 1.10255, Test Acc: 0.47%\n",
            "Epoch: 1 | Loss: 1.19986, Acc: 0.47% | Test Loss: 0.81543, Test Acc: 0.52%\n",
            "Epoch: 2 | Loss: 1.11168, Acc: 0.51% | Test Loss: 0.76398, Test Acc: 0.56%\n",
            "Training complete in 2m 42s\n",
            "Best val Acc: 0.562000\n",
            "****************************************************************************************************\n",
            "training model for a = 0.25, t = 1.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 1.76631, Acc: 0.36% | Test Loss: 1.30094, Test Acc: 0.46%\n",
            "Epoch: 1 | Loss: 1.48221, Acc: 0.47% | Test Loss: 1.19049, Test Acc: 0.51%\n",
            "Epoch: 2 | Loss: 1.37296, Acc: 0.52% | Test Loss: 1.30132, Test Acc: 0.56%\n",
            "Training complete in 2m 41s\n",
            "Best val Acc: 0.563300\n",
            "****************************************************************************************************\n",
            "training model for a = 0.25, t = 10.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 1 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 2 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Training complete in 2m 40s\n",
            "Best val Acc: 0.100000\n",
            "****************************************************************************************************\n",
            "training model for a = 0.25, t = 100.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 1 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 2 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Training complete in 2m 40s\n",
            "Best val Acc: 0.100000\n",
            "****************************************************************************************************\n",
            "training model for a = 0.5, t = 0.01\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 0.90344, Acc: 0.34% | Test Loss: 0.72488, Test Acc: 0.45%\n",
            "Epoch: 1 | Loss: 0.75536, Acc: 0.45% | Test Loss: 0.52182, Test Acc: 0.51%\n",
            "Epoch: 2 | Loss: 0.69874, Acc: 0.49% | Test Loss: 0.66463, Test Acc: 0.54%\n",
            "Training complete in 2m 40s\n",
            "Best val Acc: 0.545000\n",
            "****************************************************************************************************\n",
            "training model for a = 0.5, t = 0.1\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 0.91201, Acc: 0.34% | Test Loss: 0.53180, Test Acc: 0.45%\n",
            "Epoch: 1 | Loss: 0.76621, Acc: 0.45% | Test Loss: 0.52039, Test Acc: 0.51%\n",
            "Epoch: 2 | Loss: 0.70642, Acc: 0.49% | Test Loss: 0.55132, Test Acc: 0.55%\n",
            "Training complete in 2m 40s\n",
            "Best val Acc: 0.551400\n",
            "****************************************************************************************************\n",
            "training model for a = 0.5, t = 0.5\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 1.12305, Acc: 0.34% | Test Loss: 0.91451, Test Acc: 0.44%\n",
            "Epoch: 1 | Loss: 0.95416, Acc: 0.45% | Test Loss: 0.90431, Test Acc: 0.51%\n",
            "Epoch: 2 | Loss: 0.88662, Acc: 0.49% | Test Loss: 0.83730, Test Acc: 0.54%\n",
            "Training complete in 2m 40s\n",
            "Best val Acc: 0.535800\n",
            "****************************************************************************************************\n",
            "training model for a = 0.5, t = 1.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 1.79138, Acc: 0.35% | Test Loss: 1.75071, Test Acc: 0.46%\n",
            "Epoch: 1 | Loss: 1.52281, Acc: 0.46% | Test Loss: 1.50212, Test Acc: 0.52%\n",
            "Epoch: 2 | Loss: 1.41375, Acc: 0.51% | Test Loss: 1.52606, Test Acc: 0.55%\n",
            "Training complete in 2m 40s\n",
            "Best val Acc: 0.547000\n",
            "****************************************************************************************************\n",
            "training model for a = 0.5, t = 10.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 1 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 2 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Training complete in 2m 41s\n",
            "Best val Acc: 0.100000\n",
            "****************************************************************************************************\n",
            "training model for a = 0.5, t = 100.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 1 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 2 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Training complete in 2m 43s\n",
            "Best val Acc: 0.100000\n",
            "****************************************************************************************************\n",
            "training model for a = 0.75, t = 0.01\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 0.47335, Acc: 0.31% | Test Loss: 0.36536, Test Acc: 0.42%\n",
            "Epoch: 1 | Loss: 0.40005, Acc: 0.41% | Test Loss: 0.29802, Test Acc: 0.47%\n",
            "Epoch: 2 | Loss: 0.37383, Acc: 0.46% | Test Loss: 0.27520, Test Acc: 0.50%\n",
            "Training complete in 2m 42s\n",
            "Best val Acc: 0.500300\n",
            "****************************************************************************************************\n",
            "training model for a = 0.75, t = 0.1\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 0.49745, Acc: 0.29% | Test Loss: 0.40569, Test Acc: 0.41%\n",
            "Epoch: 1 | Loss: 0.41737, Acc: 0.41% | Test Loss: 0.30496, Test Acc: 0.47%\n",
            "Epoch: 2 | Loss: 0.38873, Acc: 0.45% | Test Loss: 0.30280, Test Acc: 0.50%\n",
            "Training complete in 2m 42s\n",
            "Best val Acc: 0.500000\n",
            "****************************************************************************************************\n",
            "training model for a = 0.75, t = 0.5\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 0.81623, Acc: 0.32% | Test Loss: 0.74733, Test Acc: 0.44%\n",
            "Epoch: 1 | Loss: 0.69487, Acc: 0.43% | Test Loss: 0.67049, Test Acc: 0.48%\n",
            "Epoch: 2 | Loss: 0.65117, Acc: 0.47% | Test Loss: 0.65702, Test Acc: 0.52%\n",
            "Training complete in 2m 42s\n",
            "Best val Acc: 0.517000\n",
            "****************************************************************************************************\n",
            "training model for a = 0.75, t = 1.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 1.85526, Acc: 0.33% | Test Loss: 2.02297, Test Acc: 0.43%\n",
            "Epoch: 1 | Loss: 1.57813, Acc: 0.44% | Test Loss: 1.94975, Test Acc: 0.49%\n",
            "Epoch: 2 | Loss: 1.47935, Acc: 0.48% | Test Loss: 1.80724, Test Acc: 0.53%\n",
            "Training complete in 2m 43s\n",
            "Best val Acc: 0.526100\n",
            "****************************************************************************************************\n",
            "training model for a = 0.75, t = 10.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 1 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 2 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Training complete in 2m 42s\n",
            "Best val Acc: 0.100000\n",
            "****************************************************************************************************\n",
            "training model for a = 0.75, t = 100.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 1 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 2 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Training complete in 2m 43s\n",
            "Best val Acc: 0.100000\n",
            "****************************************************************************************************\n",
            "training model for a = 1.0, t = 0.01\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 0.00025, Acc: 0.09% | Test Loss: 0.00025, Test Acc: 0.09%\n",
            "Epoch: 1 | Loss: 0.00025, Acc: 0.10% | Test Loss: 0.00025, Test Acc: 0.09%\n",
            "Epoch: 2 | Loss: 0.00025, Acc: 0.09% | Test Loss: 0.00025, Test Acc: 0.09%\n",
            "Training complete in 2m 45s\n",
            "Best val Acc: 0.094200\n",
            "****************************************************************************************************\n",
            "training model for a = 1.0, t = 0.1\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 0.02380, Acc: 0.12% | Test Loss: 0.02200, Test Acc: 0.16%\n",
            "Epoch: 1 | Loss: 0.02257, Acc: 0.16% | Test Loss: 0.02267, Test Acc: 0.20%\n",
            "Epoch: 2 | Loss: 0.02188, Acc: 0.19% | Test Loss: 0.02144, Test Acc: 0.22%\n",
            "Training complete in 2m 41s\n",
            "Best val Acc: 0.222000\n",
            "****************************************************************************************************\n",
            "training model for a = 1.0, t = 0.5\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 0.50481, Acc: 0.23% | Test Loss: 0.49590, Test Acc: 0.33%\n",
            "Epoch: 1 | Loss: 0.43847, Acc: 0.33% | Test Loss: 0.48984, Test Acc: 0.38%\n",
            "Epoch: 2 | Loss: 0.41418, Acc: 0.37% | Test Loss: 0.43800, Test Acc: 0.41%\n",
            "Training complete in 2m 40s\n",
            "Best val Acc: 0.412400\n",
            "****************************************************************************************************\n",
            "training model for a = 1.0, t = 1.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 1.87935, Acc: 0.30% | Test Loss: 1.98018, Test Acc: 0.40%\n",
            "Epoch: 1 | Loss: 1.62667, Acc: 0.39% | Test Loss: 2.05188, Test Acc: 0.44%\n",
            "Epoch: 2 | Loss: 1.53604, Acc: 0.43% | Test Loss: 2.06987, Test Acc: 0.46%\n",
            "Training complete in 2m 40s\n",
            "Best val Acc: 0.460600\n",
            "****************************************************************************************************\n",
            "training model for a = 1.0, t = 10.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 1 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 2 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Training complete in 2m 40s\n",
            "Best val Acc: 0.100000\n",
            "****************************************************************************************************\n",
            "training model for a = 1.0, t = 100.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 1 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Epoch: 2 | Loss: nan, Acc: 0.10% | Test Loss: nan, Test Acc: 0.10%\n",
            "Training complete in 2m 39s\n",
            "Best val Acc: 0.100000\n",
            "****************************************************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "based of the result, we chose two a and two t too see result of increasing teacher model effect on training the model:"
      ],
      "metadata": {
        "id": "Ret2Rv_5hHyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_res18_Hinton = models.resnet18(pretrained=False)\n",
        "num_ftrs = model_res18_Hinton.fc.in_features\n",
        "# Here the size of each output sample is set to 10.\n",
        "model_res18_Hinton.fc = nn.Linear(num_ftrs, 10)\n",
        "model_res18_Hinton = model_res18_Hinton.to(device)\n",
        "\n",
        "print(f\"training model for a = {a[1]}, t = {t[2]}\")\n",
        "print(100*'*')\n",
        "optimizer_ft = optim.SGD(model_res18_Hinton.parameters(), lr=0.001, momentum=0.9)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=50, gamma=0.5)\n",
        "model_res18_Hinton_trained,_,_ = train_model_manual_Hinton_augment(model_res18_Hinton,model_ft, optimizer_ft,exp_lr_scheduler,30,t[2],a[1],num_epochs=200) #a[1] = 0.25 and t[2] = 0.5 our selected parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIuyq3wACY9H",
        "outputId": "86b10118-61ec-4520-ba04-8198a9a4976c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training model for a = 0.25, t = 0.5\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 1.42751, Acc: 0.36% | Test Loss: 1.10407, Test Acc: 0.47%\n",
            "Epoch: 1 | Loss: 1.20138, Acc: 0.47% | Test Loss: 0.77541, Test Acc: 0.53%\n",
            "Epoch: 2 | Loss: 1.10536, Acc: 0.51% | Test Loss: 0.72171, Test Acc: 0.56%\n",
            "Epoch: 3 | Loss: 1.03841, Acc: 0.54% | Test Loss: 0.77159, Test Acc: 0.59%\n",
            "Epoch: 4 | Loss: 0.98146, Acc: 0.57% | Test Loss: 0.77922, Test Acc: 0.62%\n",
            "Epoch: 5 | Loss: 0.93522, Acc: 0.60% | Test Loss: 0.59126, Test Acc: 0.63%\n",
            "Epoch: 6 | Loss: 0.89648, Acc: 0.62% | Test Loss: 0.55956, Test Acc: 0.64%\n",
            "Epoch: 7 | Loss: 0.85960, Acc: 0.63% | Test Loss: 0.53435, Test Acc: 0.65%\n",
            "Epoch: 8 | Loss: 0.83812, Acc: 0.64% | Test Loss: 0.61123, Test Acc: 0.66%\n",
            "Epoch: 9 | Loss: 0.81087, Acc: 0.65% | Test Loss: 0.62541, Test Acc: 0.68%\n",
            "Epoch: 10 | Loss: 0.78490, Acc: 0.66% | Test Loss: 0.50532, Test Acc: 0.69%\n",
            "Epoch: 11 | Loss: 0.76184, Acc: 0.68% | Test Loss: 0.53583, Test Acc: 0.69%\n",
            "Epoch: 12 | Loss: 0.74255, Acc: 0.68% | Test Loss: 0.41936, Test Acc: 0.71%\n",
            "Epoch: 13 | Loss: 0.72186, Acc: 0.69% | Test Loss: 0.56724, Test Acc: 0.70%\n",
            "Epoch: 14 | Loss: 0.70183, Acc: 0.70% | Test Loss: 0.34366, Test Acc: 0.70%\n",
            "Epoch: 15 | Loss: 0.68957, Acc: 0.71% | Test Loss: 0.56428, Test Acc: 0.72%\n",
            "Epoch: 16 | Loss: 0.67141, Acc: 0.72% | Test Loss: 0.56562, Test Acc: 0.71%\n",
            "Epoch: 17 | Loss: 0.65584, Acc: 0.72% | Test Loss: 0.54085, Test Acc: 0.71%\n",
            "Epoch: 18 | Loss: 0.64209, Acc: 0.73% | Test Loss: 0.56341, Test Acc: 0.72%\n",
            "Epoch: 19 | Loss: 0.62794, Acc: 0.74% | Test Loss: 0.50697, Test Acc: 0.73%\n",
            "Epoch: 20 | Loss: 0.61578, Acc: 0.74% | Test Loss: 0.63262, Test Acc: 0.73%\n",
            "Epoch: 21 | Loss: 0.60295, Acc: 0.75% | Test Loss: 0.56078, Test Acc: 0.73%\n",
            "Epoch: 22 | Loss: 0.58560, Acc: 0.76% | Test Loss: 0.50953, Test Acc: 0.74%\n",
            "Epoch: 23 | Loss: 0.57879, Acc: 0.76% | Test Loss: 0.50009, Test Acc: 0.74%\n",
            "Epoch: 24 | Loss: 0.56835, Acc: 0.76% | Test Loss: 0.56115, Test Acc: 0.74%\n",
            "Epoch: 25 | Loss: 0.55548, Acc: 0.77% | Test Loss: 0.36065, Test Acc: 0.74%\n",
            "Epoch: 26 | Loss: 0.54727, Acc: 0.78% | Test Loss: 0.44210, Test Acc: 0.75%\n",
            "Epoch: 27 | Loss: 0.53661, Acc: 0.78% | Test Loss: 0.45993, Test Acc: 0.74%\n",
            "Epoch: 28 | Loss: 0.52497, Acc: 0.78% | Test Loss: 0.33272, Test Acc: 0.75%\n",
            "Epoch: 29 | Loss: 0.51773, Acc: 0.79% | Test Loss: 0.41603, Test Acc: 0.75%\n",
            "Epoch: 30 | Loss: 0.50801, Acc: 0.79% | Test Loss: 0.46980, Test Acc: 0.75%\n",
            "Epoch: 31 | Loss: 0.49864, Acc: 0.80% | Test Loss: 0.44701, Test Acc: 0.76%\n",
            "Epoch: 32 | Loss: 0.49084, Acc: 0.80% | Test Loss: 0.44759, Test Acc: 0.75%\n",
            "Epoch: 33 | Loss: 0.47794, Acc: 0.81% | Test Loss: 0.39377, Test Acc: 0.76%\n",
            "Epoch: 34 | Loss: 0.47240, Acc: 0.81% | Test Loss: 0.32844, Test Acc: 0.75%\n",
            "Epoch: 35 | Loss: 0.46886, Acc: 0.81% | Test Loss: 0.29782, Test Acc: 0.76%\n",
            "Epoch: 36 | Loss: 0.45595, Acc: 0.82% | Test Loss: 0.41897, Test Acc: 0.75%\n",
            "Epoch: 37 | Loss: 0.45148, Acc: 0.82% | Test Loss: 0.45186, Test Acc: 0.75%\n",
            "Epoch: 38 | Loss: 0.43718, Acc: 0.83% | Test Loss: 0.42912, Test Acc: 0.75%\n",
            "Epoch: 39 | Loss: 0.42962, Acc: 0.83% | Test Loss: 0.27022, Test Acc: 0.76%\n",
            "Epoch: 40 | Loss: 0.42932, Acc: 0.83% | Test Loss: 0.31961, Test Acc: 0.76%\n",
            "Epoch: 41 | Loss: 0.42036, Acc: 0.84% | Test Loss: 0.47473, Test Acc: 0.76%\n",
            "Epoch: 42 | Loss: 0.41213, Acc: 0.84% | Test Loss: 0.33255, Test Acc: 0.77%\n",
            "Epoch: 43 | Loss: 0.40637, Acc: 0.84% | Test Loss: 0.24431, Test Acc: 0.76%\n",
            "Epoch: 44 | Loss: 0.40138, Acc: 0.84% | Test Loss: 0.33870, Test Acc: 0.76%\n",
            "Epoch: 45 | Loss: 0.39102, Acc: 0.85% | Test Loss: 0.29952, Test Acc: 0.76%\n",
            "Epoch: 46 | Loss: 0.38287, Acc: 0.85% | Test Loss: 0.37221, Test Acc: 0.76%\n",
            "Epoch: 47 | Loss: 0.37895, Acc: 0.85% | Test Loss: 0.51102, Test Acc: 0.76%\n",
            "Epoch: 48 | Loss: 0.37471, Acc: 0.85% | Test Loss: 0.58294, Test Acc: 0.76%\n",
            "Epoch: 49 | Loss: 0.37045, Acc: 0.86% | Test Loss: 0.55817, Test Acc: 0.76%\n",
            "Epoch: 50 | Loss: 0.33358, Acc: 0.88% | Test Loss: 0.45549, Test Acc: 0.77%\n",
            "Epoch: 51 | Loss: 0.31873, Acc: 0.88% | Test Loss: 0.45566, Test Acc: 0.77%\n",
            "Epoch: 52 | Loss: 0.30895, Acc: 0.89% | Test Loss: 0.44380, Test Acc: 0.77%\n",
            "Epoch: 53 | Loss: 0.30370, Acc: 0.89% | Test Loss: 0.42758, Test Acc: 0.77%\n",
            "Epoch: 54 | Loss: 0.30359, Acc: 0.89% | Test Loss: 0.39264, Test Acc: 0.77%\n",
            "Epoch: 55 | Loss: 0.29777, Acc: 0.89% | Test Loss: 0.29182, Test Acc: 0.77%\n",
            "Epoch: 56 | Loss: 0.29588, Acc: 0.89% | Test Loss: 0.49974, Test Acc: 0.77%\n",
            "Epoch: 57 | Loss: 0.28974, Acc: 0.90% | Test Loss: 0.44423, Test Acc: 0.77%\n",
            "Epoch: 58 | Loss: 0.28854, Acc: 0.90% | Test Loss: 0.40171, Test Acc: 0.77%\n",
            "Epoch: 59 | Loss: 0.28418, Acc: 0.90% | Test Loss: 0.49687, Test Acc: 0.77%\n",
            "Epoch: 60 | Loss: 0.28274, Acc: 0.90% | Test Loss: 0.42392, Test Acc: 0.77%\n",
            "Epoch: 61 | Loss: 0.27844, Acc: 0.90% | Test Loss: 0.36189, Test Acc: 0.77%\n",
            "Epoch: 62 | Loss: 0.27456, Acc: 0.91% | Test Loss: 0.50049, Test Acc: 0.78%\n",
            "Epoch: 63 | Loss: 0.27162, Acc: 0.91% | Test Loss: 0.45423, Test Acc: 0.78%\n",
            "Epoch: 64 | Loss: 0.27124, Acc: 0.91% | Test Loss: 0.41345, Test Acc: 0.77%\n",
            "Epoch: 65 | Loss: 0.26598, Acc: 0.91% | Test Loss: 0.43240, Test Acc: 0.77%\n",
            "Epoch: 66 | Loss: 0.26605, Acc: 0.91% | Test Loss: 0.45525, Test Acc: 0.77%\n",
            "Epoch: 67 | Loss: 0.26053, Acc: 0.91% | Test Loss: 0.41391, Test Acc: 0.78%\n",
            "Epoch: 68 | Loss: 0.25748, Acc: 0.92% | Test Loss: 0.57835, Test Acc: 0.77%\n",
            "Epoch: 69 | Loss: 0.25405, Acc: 0.92% | Test Loss: 0.31726, Test Acc: 0.77%\n",
            "Epoch: 70 | Loss: 0.24998, Acc: 0.92% | Test Loss: 0.45829, Test Acc: 0.77%\n",
            "Epoch: 71 | Loss: 0.25082, Acc: 0.92% | Test Loss: 0.36250, Test Acc: 0.77%\n",
            "Epoch: 72 | Loss: 0.24644, Acc: 0.92% | Test Loss: 0.31953, Test Acc: 0.77%\n",
            "Epoch: 73 | Loss: 0.24584, Acc: 0.92% | Test Loss: 0.39911, Test Acc: 0.78%\n",
            "early stopping happend!\n",
            "Training complete in 68m 17s\n",
            "Best val Acc: 0.777200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_res18_Hinton = models.resnet18(pretrained=False)\n",
        "num_ftrs = model_res18_Hinton.fc.in_features\n",
        "# Here the size of each output sample is set to 10.\n",
        "model_res18_Hinton.fc = nn.Linear(num_ftrs, 10)\n",
        "model_res18_Hinton = model_res18_Hinton.to(device)\n",
        "\n",
        "print(f\"training model for a = {a[2]}, t = {t[3]}\")\n",
        "print(100*'*')\n",
        "optimizer_ft = optim.SGD(model_res18_Hinton.parameters(), lr=0.001, momentum=0.9)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=50, gamma=0.5)\n",
        "model_res18_Hinton_trained,_,_ = train_model_manual_Hinton_augment(model_res18_Hinton,model_ft, optimizer_ft,exp_lr_scheduler,30,t[2],a[1],num_epochs=200) #a[1] = 0.25 and t[2] = 0.5 our selected parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koeRb9e1lRln",
        "outputId": "63f3df66-dc84-4391-a51e-9706610b7a3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training model for a = 0.5, t = 1.0\n",
            "****************************************************************************************************\n",
            "Epoch: 0 | Loss: 1.42459, Acc: 0.36% | Test Loss: 0.88821, Test Acc: 0.47%\n",
            "Epoch: 1 | Loss: 1.19836, Acc: 0.47% | Test Loss: 0.77649, Test Acc: 0.52%\n",
            "Epoch: 2 | Loss: 1.10129, Acc: 0.52% | Test Loss: 0.68140, Test Acc: 0.57%\n",
            "Epoch: 3 | Loss: 1.02855, Acc: 0.55% | Test Loss: 0.80662, Test Acc: 0.58%\n",
            "Epoch: 4 | Loss: 0.97624, Acc: 0.58% | Test Loss: 0.53851, Test Acc: 0.60%\n",
            "Epoch: 5 | Loss: 0.93034, Acc: 0.60% | Test Loss: 0.54544, Test Acc: 0.64%\n",
            "Epoch: 6 | Loss: 0.89449, Acc: 0.61% | Test Loss: 0.42693, Test Acc: 0.64%\n",
            "Epoch: 7 | Loss: 0.86198, Acc: 0.63% | Test Loss: 0.52531, Test Acc: 0.67%\n",
            "Epoch: 8 | Loss: 0.82671, Acc: 0.65% | Test Loss: 0.49347, Test Acc: 0.68%\n",
            "Epoch: 9 | Loss: 0.80023, Acc: 0.66% | Test Loss: 0.65856, Test Acc: 0.68%\n",
            "Epoch: 10 | Loss: 0.77603, Acc: 0.67% | Test Loss: 0.49650, Test Acc: 0.69%\n",
            "Epoch: 11 | Loss: 0.75238, Acc: 0.68% | Test Loss: 0.50955, Test Acc: 0.70%\n",
            "Epoch: 12 | Loss: 0.73057, Acc: 0.69% | Test Loss: 0.51441, Test Acc: 0.70%\n",
            "Epoch: 13 | Loss: 0.71378, Acc: 0.70% | Test Loss: 0.64969, Test Acc: 0.72%\n",
            "Epoch: 14 | Loss: 0.69745, Acc: 0.70% | Test Loss: 0.55575, Test Acc: 0.72%\n",
            "Epoch: 15 | Loss: 0.68483, Acc: 0.71% | Test Loss: 0.51408, Test Acc: 0.71%\n",
            "Epoch: 16 | Loss: 0.66563, Acc: 0.72% | Test Loss: 0.46446, Test Acc: 0.72%\n",
            "Epoch: 17 | Loss: 0.65307, Acc: 0.73% | Test Loss: 0.48644, Test Acc: 0.72%\n",
            "Epoch: 18 | Loss: 0.64091, Acc: 0.73% | Test Loss: 0.47768, Test Acc: 0.73%\n",
            "Epoch: 19 | Loss: 0.62405, Acc: 0.74% | Test Loss: 0.35345, Test Acc: 0.73%\n",
            "Epoch: 20 | Loss: 0.61426, Acc: 0.74% | Test Loss: 0.45673, Test Acc: 0.74%\n",
            "Epoch: 21 | Loss: 0.59851, Acc: 0.75% | Test Loss: 0.51779, Test Acc: 0.73%\n",
            "Epoch: 22 | Loss: 0.58583, Acc: 0.76% | Test Loss: 0.42105, Test Acc: 0.74%\n",
            "Epoch: 23 | Loss: 0.57660, Acc: 0.76% | Test Loss: 0.54937, Test Acc: 0.74%\n",
            "Epoch: 24 | Loss: 0.56120, Acc: 0.77% | Test Loss: 0.57813, Test Acc: 0.75%\n",
            "Epoch: 25 | Loss: 0.55567, Acc: 0.77% | Test Loss: 0.55325, Test Acc: 0.75%\n",
            "Epoch: 26 | Loss: 0.54774, Acc: 0.77% | Test Loss: 0.47147, Test Acc: 0.74%\n",
            "Epoch: 27 | Loss: 0.53717, Acc: 0.78% | Test Loss: 0.38928, Test Acc: 0.75%\n",
            "Epoch: 28 | Loss: 0.52595, Acc: 0.78% | Test Loss: 0.49650, Test Acc: 0.75%\n",
            "Epoch: 29 | Loss: 0.51707, Acc: 0.79% | Test Loss: 0.46324, Test Acc: 0.76%\n",
            "Epoch: 30 | Loss: 0.50785, Acc: 0.79% | Test Loss: 0.54108, Test Acc: 0.75%\n",
            "Epoch: 31 | Loss: 0.49606, Acc: 0.80% | Test Loss: 0.70149, Test Acc: 0.76%\n",
            "Epoch: 32 | Loss: 0.48980, Acc: 0.80% | Test Loss: 0.76910, Test Acc: 0.76%\n",
            "Epoch: 33 | Loss: 0.48382, Acc: 0.80% | Test Loss: 0.64266, Test Acc: 0.76%\n",
            "Epoch: 34 | Loss: 0.47464, Acc: 0.81% | Test Loss: 0.40549, Test Acc: 0.76%\n",
            "Epoch: 35 | Loss: 0.46661, Acc: 0.81% | Test Loss: 0.36383, Test Acc: 0.76%\n",
            "Epoch: 36 | Loss: 0.45921, Acc: 0.82% | Test Loss: 0.52346, Test Acc: 0.76%\n",
            "Epoch: 37 | Loss: 0.44639, Acc: 0.82% | Test Loss: 0.55719, Test Acc: 0.76%\n",
            "Epoch: 38 | Loss: 0.44552, Acc: 0.82% | Test Loss: 0.49478, Test Acc: 0.76%\n",
            "Epoch: 39 | Loss: 0.43224, Acc: 0.83% | Test Loss: 0.49447, Test Acc: 0.77%\n",
            "Epoch: 40 | Loss: 0.42613, Acc: 0.83% | Test Loss: 0.70143, Test Acc: 0.77%\n",
            "Epoch: 41 | Loss: 0.42065, Acc: 0.83% | Test Loss: 0.70285, Test Acc: 0.77%\n",
            "Epoch: 42 | Loss: 0.41089, Acc: 0.84% | Test Loss: 0.38298, Test Acc: 0.77%\n",
            "Epoch: 43 | Loss: 0.40634, Acc: 0.84% | Test Loss: 0.51792, Test Acc: 0.77%\n",
            "Epoch: 44 | Loss: 0.39960, Acc: 0.84% | Test Loss: 0.71985, Test Acc: 0.77%\n",
            "Epoch: 45 | Loss: 0.39664, Acc: 0.84% | Test Loss: 0.47858, Test Acc: 0.77%\n",
            "Epoch: 46 | Loss: 0.39070, Acc: 0.85% | Test Loss: 0.52160, Test Acc: 0.76%\n",
            "Epoch: 47 | Loss: 0.38247, Acc: 0.85% | Test Loss: 0.81280, Test Acc: 0.77%\n",
            "Epoch: 48 | Loss: 0.37733, Acc: 0.86% | Test Loss: 0.52666, Test Acc: 0.77%\n",
            "Epoch: 49 | Loss: 0.36929, Acc: 0.86% | Test Loss: 0.56715, Test Acc: 0.77%\n",
            "early stopping happend!\n",
            "Training complete in 44m 58s\n",
            "Best val Acc: 0.783700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "as we can see in this part, if techer model have more information about correct labels ( in part D we achived accuracy = %86 which is far better than %76 of resnet18) it can help us to achive better result, in our case now we can achive accuracy value of 78, and as estated befor using teacher model also help us to have a faster convergence rate ( because of high level information stored in it's logits).\n",
        "\n",
        "in conclusion: using teacher model even when it doesn't have a high accuracy, can help us to train model faster, but if teacher model have a higher accuracy as well, it can help to train  model faster and achive better result than just training model without any extra information."
      ],
      "metadata": {
        "id": "3tXSikxOhWs2"
      }
    }
  ]
}